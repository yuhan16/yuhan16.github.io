<!DOCTYPE html>
<html>
    <head>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width initial-scale=1">

<meta property="og:title" content="Continuous Randon Variables">
<title>Continuous Randon Variables</title>
<meta property="og:description" content="This note reviews the concepts of continuous random variables.">
<meta property="og:url" content="http://localhost:4000/2020/10/notes-3.html">
<meta property="og:site_name" content="HuskyDev">
<meta property="og:locale" content="">

<meta name="keywords" content="YuhanHuskyDev">

<link rel="icon" href="/assets/images/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="http://localhost:4000/2020/10/notes-3.html">
<link rel="alternate" type="application/atom+xml" title="HuskyDev" href="http://localhost:4000/feed.xml" />

<!--script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script-->
<script src="https://kit.fontawesome.com/e49cc00366.js" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css">


<link rel="stylesheet" href="/assets/css/github.min.css">
<script src="/assets/js/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ['\\(','\\)'] ],
        displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ],
        processEscapes: true
      },
      "HTML-CSS": { scale: 90 }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!--script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script-->

<script src="/assets/js/myjavascript.js"></script>
    
</head>




<body>
    
    <nav class="navbar">
    <div class="navbar-container">
        <!--Navbar logo-->
        <a href="/"><img src="/assets/images/husky_logo.png" class="navbar-logo"></a>

        <!--Navbar menu-->
        <ul class="main-menu">
            
                
                    <li><a href="/">Home</a></li>
                
            
                
                    <li><a href="/blog">Blog</a></li>
                
            
                
                    <li><a href="/publications">Publications</a></li>
                
            
                
                    <li><a href="/categories">Categories</a></li>
                
            
                
                    <li><a href="/tags">Tags</a></li>
                
            
                
                    <li><a href="/archives">Archives</a></li>
                
            
            
            <li><a href="javascript:void(0)"><i class="fa fa-search"></i></a></li>
        </ul>
    </div>
    </nav>


	<div class="card-container"><!--default_1 is single card layout, sutiable for pages like aboutme, tags, categories, require only one card to display content-->
        <div class="card">
            <div class="post">
    <h1 class="post-title">Continuous Randon Variables</h1>
	
	<!--pose meta data-->
	<div style="display: flex; gap: 10px; margin-bottom: 1rem;">
		<span class="post-meta">
			<i class="fa-regular fa-calendar-check">&nbsp;</i>2020-10-16
		</span>

		<span class="post-meta">
			<i class="fa-regular fa-folder-open"></i>&nbsp;<a href="/categories/probability">Probability</a>
		</span>

		<span class="post-meta">
			<i class="fa-regular fa-clock"></i>&nbsp;
		</span>

        <!--span class="post-meta">
			<i class="fas fa-tags"></i>&nbsp;
            
                
                <a href="/tags/#notes">notes,</a>
                
            
                
                <a href="/tags/#math">math</a>
                
            
		</span-->
	</div>

	<!--table of content-->
	<div class="toc">
		<p class="toc-meta">Table of Contents</p>
		<div class="toc-content">
			
		</div>
	</div>
    
	<!--post content-->
    <article class="post-content">
        <h2 id="probability-mass-functions">Probability mass functions</h2>
<div class="theorem"><p><b>Definition 1.</b> 
The <b>(probability) mass function</b> (p.m.f.) of a discrete random variable $X$ is the function $f: \mathbb{R} \to[0, 1]$ given by $f(x) = \mathbf{P}(X = x)$. 
</p></div>

<p>Notice that $f(x)$ is not discontinuous. The distribution and mass functions are related by
\(\begin{equation}
    F(x) = \sum_{i: x_i \leq x} f(x_i), \quad f(x) = F(x) - \lim_{y\uparrow x} F(y).
\end{equation}\)</p>

<div class="theorem"><p><b>Lemma.</b> 
The probability mass function $f: \mathbb{R} \to [0, 1]$ satisfies: 
<ol>
    <li>the set of $x$ such that $f(x) \neq 0$ is countable,</li>
    <li>$\sum_{i} f(x_i) = 1$, where $x_1, x_2, \dots$ are the values of $x$ such that $f(x)\neq 0$.</li>
</ol>
</p></div>

<div class="example"><p><b>Example.</b> 
<b>Binomial distribution.</b> A coin is tossed $n$ times, and a head turns up each time with probability $p (= 1 - q)$. Then $\Omega = \{H, T\}^n \}$. The total number $X$ of heads takes values in the set $\{0, 1, 2, \dots , n\}$ and is a discrete random variable. Its probability mass function $f(x) = \mathbf{P}(X = x)$ satisfies 
$$\begin{equation}
    f(k)= \binom{n}{k} p^{k}(1-p)^{n-k}, \quad k=0,1, \ldots, n.
\end{equation}$$
The random variable $X$ is said to have the <b>binomial distribution</b> with parameters $n$ and $p$, It is the sum $X = Y_1 + Y_2 + \dots + Y_n$ of $n$ Bernoulli variables.
</p></div>

<div class="example"><p><b>Example.</b> 
<b>Poisson distribution.</b> If a random variable $X$ takes values in the set $\{O, 1, 2, \dots \}$ with mass function
$$\begin{equation}
    f(k) = \frac{\lambda^k}{k!} e^{-\lambda}, \quad k = 0, 1, 2, \dots, 
\end{equation}$$
where $\lambda &gt; 0$, then $X$ is said to have the <b>Poisson distribution</b> with parameter $\lambda$. Figure 1. shows how p.m.f varies with $k$.
</p>
<figure>
    <img src="/assets/images/blog/2020/2020-10-16-poisson.png" height="230" />
    <figcaption>Fig.1: p.m.f of Poisson distribution.</figcaption>
</figure>
</div>

<div class="remark"><p><b>Remark.</b> 
The interpretation of Poisson distribution can be found via this link: <a href="https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459">The Poisson Distribution and Poisson Process Explained</a>.
</p></div>

<h2 id="independence-of-discrete-random-variables">Independence of discrete random variables</h2>
<p>Recall that events $A$ and $B$ are called independent if and only if $\mathbf{P}(A \cap B) = \mathbf{P}(A)\mathbf{P}(B)$.</p>

<div class="theorem"><p><b>Definition 2.</b> 
Discrete variables $X$ and $Y$ are <b>independent</b> if the events $\{X = x\}$ and $\{Y = y\}$ are independent for <b>all</b> $x$ and $y$. 
</p></div>

<p>Let $A: = { \omega \;\vert\; X(\omega) = x }$ and $B:= { \omega \;\vert\; Y(\omega) = y }$. Then we can write $P(A) = f_X(x), P(B) = f_Y(y)$ and $P(A \cap B) = f(x, y)$. Therefore, $A$ and $B$ are independent if and only if $f(x,y) = f_X(x) f_Y(y)$ for all $x$ and $y$.</p>

<div class="remark"><p><b>Remark.</b> 
The equality $f(x,y) = f_X(x) f_Y(y)$ can be used as the criterion to determine whether two discrete random variables $X$ and $Y$ are independent or not. But <b>we need to be careful with it when dealing with continuous random variables</b> as there will be additional assumptions to determine the independence of continuous random variables using this equality.
</p></div>

<div class="theorem"><p><b>Theorem 1.</b> 
If $X$ and $Y$ are independent and $g, h: \mathbb{R} \to \mathbb{R}$, then $g(X)$ and $h(Y)$ are independent also. 
</p></div>

<p>More generally, we say that a family ${ X_i \;\vert\; i \in I}$ of (discrete) random variables is independent if the events ${X_i = x_i }, i \in I$, are independent for all possible choices of the set ${x_i \;\vert\; i \in I}$ of the values of the $X_i$ . That is to say, ${X_i \;\vert\; i \in I }$ is an independent family if and only if 
\(\begin{equation}
    \mathbf{P}(X_i = x_i \text{ for all }i\in J) = \prod_{i\in J} \mathbf{P}(X_i = x_i)
\end{equation}\)
for all sets ${x_i \;\vert\; i \in I}$ and for all finite subsets $J$ of $I$.</p>

<h2 id="probability-density-functions">Probability density functions</h2>
<p>Recall that a random variable $X$ is continuous if its distribution function $F(x) = \mathbf{P}(X \leq x)$ can be written as\footnote{This is just a general integral, $f(u)$ may or may not be continuous.}
\(\begin{equation}
    F(x) = \int_{\infty}^x f(u)du
\end{equation}\)
for some integrable $f: \mathbb{R} \to [0, \infty)$.</p>

<div class="theorem"><p><b>Definition 3.</b> 
The function $f$ is called the <b>(probability) density function</b> (p.d.f.) of the continuous random variable $X$. 
</p></div>

<div class="remark"><p><b>Remark.</b> 
The function $f$ is <b>NOT</b> unique. We can add some separate points or a countable set of points which has zero measure to $f$. This doesn't change the value of the integral. However, if $F$ is differentiable at $u$ then we shall normally set $f(u) = F'(u)$. 
</p></div>

<p>Next, we assume $f$ is continuous, then from the basic theorem of calculus, $F$ must be differentiable. Recall 
\(\begin{equation}
    P(a &lt; X(\omega) \leq b) = F(b) - F(a) = \int_{a}^b f(u)du.
\end{equation}\)
Then $P(X=x) = F(x) - \lim_{y \uparrow x} F(y)$. $F$ is <strong>absolutely continuous</strong> for continuous random variables. Thus we have
\(\begin{equation}
    \lim_{y\uparrow x} F(y) = F(x) \quad \mathbb{R}ightarrow \quad P(X=x) = 0 \quad \forall x\in\mathbb{R}.
\end{equation}\)
This means <strong>the probability of a continuous random variable $X$ taking value at a certain point is 0</strong>. Very roughly speaking, this lies in the observation that there are uncountably many possible values for $X$; this number is so large that the probability of $X$ taking any particular value cannot exceed zero.</p>

<p>The numerical value $f(x)$ is <strong>NOT</strong> a probability. Check the probability $\mathbf{P}(x &lt; X \leq x + dx)$ for a very small $dx$:
\(\begin{equation}
    \mathbf{P}(x &lt; X \leq x + dx) = F(x + dx) - F(x) \approx f(x) dx. 
\end{equation}\)
Since $dx$ is a very small interval rather a number, we cannot say $f(x)$ is the probability of something.</p>

<div class="theorem"><p><b>Lemma.</b> 
If $X$ has density function $f$, then
<ol>
    <li> $\int_{-\infty}^\infty f(x)dx = 1$,</li>
    <li> $\mathbf{P}(X=x) = 0$ for all $x \in \mathbb{R}$,</li>
    <li> $\mathbf{P}(a &lt; X \leq b) = \int_a^b f(x)dx$.</li>
</ol>
</p></div>

<h2 id="independence-of-continuous-random-variables">Independence of continuous random variables</h2>
<h3 id="independence-of-general-random-variables">Independence of general random variables</h3>
<div class="theorem"><p><b>Definition 4.</b> 
Random variables $X$ and $Y$ are called <b>independent</b> if $\{ X \leq x\}$ and $\{Y \leq y\}$ are independent events for all $x, y \in \mathbb{R}$. 
</p></div>
<p>Note that this definition is the general definition of the independence of any two variables $X$ and $Y$ regardless of their types. The independence of discrete random variables is included in this definition.</p>

<p>Recall the marginalization. If two random variables $X$ and $Y$ are independent, we have
\(\begin{equation}
    \mathbf{P}(X\leq x) = \lim_{y\to\infty}F(x, y) \equiv F_X(x), \quad 
    \mathbf{P}(Y \leq y) = \lim_{x\to\infty}F(x, y) \equiv F_Y(y).
\end{equation}\)
Therefore,
\(\begin{equation}
    \tag{1}
    F(x,y) = \mathbf{P}(X \leq x, Y \leq y) = \mathbf{P}(X\leq x) \mathbf{P}(Y \leq y) = F_X(x) F_Y(y). 
\end{equation}\)
Note that we are dealing with distribution function in Eq.(1). Eq.(1) can be used as the general criterion to determine whether two random variables are independent or not.</p>

<h3 id="independence-of-continuous-variables">Independence of continuous variables</h3>
<p>If $X, Y$ are continuous, then we have
\(\begin{equation}
    F(x, y) = \int_{-\infty}^x \int_{-\infty}^y f(x,y) dx dy.
\end{equation}\)
\(\begin{equation}
    F_X(x) = \int_{-\infty}^x \int_{-\infty}^\infty f(x, y)dx dy \quad \mathbb{R}ightarrow \quad f_X(x) = \int_{-\infty}^\infty f(x,y)dy,
\end{equation}\)
where $f_X(x)$ is called the <strong>marginal probability density function</strong> of $X$. Similarly, we can define $F_Y(y)$ and $f_Y(y)$.</p>

<p>Assume $f(x,y)$ is continuous, then $F(x,y)$ must be twice differentiable w.r.t. $x$ and $y$. Therefore, from \eqref{eq:<em>}, we can derive a *very practical criterion</em> to determine the independence of two continuous random variables:
\(\begin{equation}
    \tag{2}
    f(x,y) = \frac{\partial^2 F(x,y)}{\partial x \partial y} = \frac{\partial}{\partial x \partial y} F_X(x) F_Y(y) = f_X(x) f_Y(y).
\end{equation}\)</p>

<div class="remark"><p><b>Remark.</b> 
Note that the prerequisite of Eq.(2) is that $f(x,y)$ is continuous.
</p></div>

<div class="example"><p><b>Example.</b> 
<b>Uniform distribution.</b> The random variable $X$ is uniform on $[a, b]$ if it has distribution function
$$\begin{equation}
    F(x) = \begin{cases} 
        0 &amp; x \leq x, \\ \frac{x-a}{b-a} &amp; a &lt; x \leq b, \\ 1 &amp; x &gt; b
    \end{cases}.
\end{equation}$$
The density function is
$$\begin{equation}
    f(x) = \begin{cases} \frac{1}{b-a} &amp; a &lt; x \leq b \\ 0 &amp; o.w. \end{cases}
\end{equation}$$
</p></div>

<div class="example"><p><b>Example.</b> 
<b>Exponential distribution</b>. The random variable $X$ is exponential with parameter $\lambda(&gt; 0)$ if it has distribution function 
$$\begin{equation}
    F(x) = 1 - e^{-\lambda x}, \quad x \geq 0.
\end{equation}$$
The density function is
$$\begin{equation}
    f(x) = \begin{cases} \lambda e^{-\lambda x} &amp; x &gt; 0 \\ 0 &amp; o.w.
    \end{cases}
\end{equation}$$
Note that $F(x)$ is not differentiable at $x=0$. This means $f$ has discontinuity at $x=0$. Thus we need to choose some value for $f(0)$. It doesn't matter what value we choose as it doesn't affect the integral. Figure 2 shows the p.d.f. of Exponential distribution.
<figure>
    <img src="/assets/images/blog/2020/2020-10-16-exponential.png" height="300" />
    <figcaption>Fig.2: p.m.f of exponential distribution.</figcaption>
</figure>
</p></div>

<div class="remark"><p><b>Remark.</b> 
The interpretation of Exponential distribution can be found via <a href="https://www.probabilitycourse.com/chapter4/4_2_2_exponential.php">Exponential distribution 1</a> and <a href="https://www.statlect.com/probability-distributions/exponential-distribution">Exponential distribution 2</a>. Pay attention to the connection between Exponential distribution and Poisson distribution.
</p></div>

<div class="example"><p><b>Example.</b> 
<b>Normal (or Gaussian) distribution.</b> The most important continuous distribution, which has two parameters $\mu$ and $\sigma^2$ and density function
$$\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x-\mu)^2}{2\sigma^2} \right), \quad x\in\mathbb{R}. 
\end{equation}$$
It is denoted by $N(\mu, \sigma^2)$. If $\mu=0$ and $\sigma^2=1$, then
$$\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}, \quad x\in\mathbb{R}.
\end{equation}$$
is the density of the standard normal distribution. It is easy to generalize 2D case to multivariable case. Suppose $\mathbf{x} \in \mathbb{R}^n$, $\mathbf{\mu} \in \mathbb{R}^n$ is the mean vector and $\mathbf{\sigma}^2 \in \mathbb{R}^{n\times n}$ is the covariance matrix. Then
$$\begin{equation}
    f(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^n} \det(\mathbf{\sigma}^2)} \exp \left( -\frac{1}{2} (\mathbf{x}-\mathbf{\mu})^T (\mathbf{\sigma}^2)^{-1} (\mathbf{x}-\mathbf{\mu}) \right), \quad \mathbf{x} \in \mathbb{R}^n.
\end{equation}$$
</p></div>

<div class="remark"><p><b>Remark.</b> 
For 2D case, if $\sigma^2$ is diagonal, then $f(x,y) = f(x)f(y)$. Therefore, $\sigma^2$ is a measure of independence. For multivariable cases, if $\sigma^2$ is diagonal, then all random variables are independent with each other.
</p></div>


    </article>

    <!--additional addons-->
    <hr style="height: 1px; margin: 1rem 0">
    <!--span class="level-item"><i class="far fa-clock"></i>&nbsp;8 minutes read (About 1210 words)</span-->
	<div class="post-meta" style="display: flex; justify-content: space-between; align-items: center;">
		<span>
			<i class="fas fa-tags"></i>&nbsp;
            
                
                <a href="/tags/notes">notes,</a>
                
            
                
                <a href="/tags/math">math</a>
                
            
		</span>
	</div>

</div>


<section class="post-pagination">
    
        <a href="/2020/10/notes-2.html">Previous: Random Variables</a>
    

    
        <a href="/2020/10/notes-4.html">Next: Expectation and Dependence</a>
    
</section>
  

<a onclick="topFunction()" id="back-top-button"><i class="fa-solid fa-chevron-up fa-2x"></i></a>


<script>
let mybutton = document.getElementById("back-top-button");	// get the button
window.onscroll = function() {scrollFunction()};	// show the button when scrolls down 200px from the top

function scrollFunction() {
  	if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
    	mybutton.style.display = "block";
  	} 
	else {
    	mybutton.style.display = "none";
  	}
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  	document.body.scrollTop = 0;
  	document.documentElement.scrollTop = 0;
}
</script>


<script>
    var coll = document.getElementsByClassName("toc-meta");
    var i;
    
    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("toc-active");
        var content = this.nextElementSibling;  	// Note the collapsible content should right after the collapsible class.
        if (content.style.maxHeight){
          	content.style.maxHeight = null;
        } else {
        	content.style.maxHeight = content.scrollHeight + "px";
        } 
      });
    }
    </script>
        </div>
	</div>

    <footer class="footerbar">
    <div class="footerbar-container footerbar-content">
        &copy; 2024 <a href="/">HuskyDev</a> 
        <span style="padding-left:30px"></span>
        Powered by <a href="https://jekyllrb.com/">Jekyll</a>
    </div>
</footer>
</body>

</html>
