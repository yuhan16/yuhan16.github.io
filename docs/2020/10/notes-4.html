<!DOCTYPE html>
<html>
    <head>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width initial-scale=1">

<meta property="og:title" content="Expectation and Dependence">
<title>Expectation and Dependence</title>
<meta property="og:description" content="This note reviews the concepts of expectation and dependence of random variables.">
<meta property="og:url" content="http://localhost:4000/2020/10/notes-4.html">
<meta property="og:site_name" content="HuskyDev">
<meta property="og:locale" content="">

<meta name="keywords" content="YuhanHuskyDev">

<link rel="icon" href="/assets/images/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="http://localhost:4000/2020/10/notes-4.html">
<link rel="alternate" type="application/atom+xml" title="HuskyDev" href="http://localhost:4000/feed.xml" />

<!--script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script-->
<script src="https://kit.fontawesome.com/e49cc00366.js" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css">


<link rel="stylesheet" href="/assets/css/github.min.css">
<script src="/assets/js/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ['\\(','\\)'] ],
        displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ],
        processEscapes: true
      },
      "HTML-CSS": { scale: 90 }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!--script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script-->

<script src="/assets/js/myjavascript.js"></script>
    
</head>




<body>
    
    <nav class="navbar">
    <div class="navbar-container">
        <!--Navbar logo-->
        <a href="/"><img src="/assets/images/husky_logo.png" class="navbar-logo"></a>

        <!--Navbar menu-->
        <ul class="main-menu">
            
                
                    <li><a href="/">Home</a></li>
                
            
                
                    <li><a href="/blog">Blog</a></li>
                
            
                
                    <li><a href="/publications">Publications</a></li>
                
            
                
                    <li><a href="/archives">Archives</a></li>
                
            
            
            <li><a href="javascript:void(0)"><i class="fa fa-search"></i></a></li>
        </ul>
    </div>
    </nav>


	<div class="card-container"><!--default_1 is single card layout, sutiable for pages like aboutme, tags, categories, require only one card to display content-->
        <div class="card">
            <div class="post">
    <h1 class="post-title">Expectation and Dependence</h1>
	
	<!--pose meta data-->
	<div style="display: flex; gap: 10px; margin-bottom: 1rem;">
		<span class="post-meta">
			<i class="fa-regular fa-calendar-check">&nbsp;</i>2020-10-23
		</span>

		<span class="post-meta">
			<i class="fa-regular fa-folder-open"></i>&nbsp;<a href="/categories/probability">Probability</a>
		</span>

		<span class="post-meta">
			<i class="fa-regular fa-clock"></i>&nbsp;
		</span>

        <!--span class="post-meta">
			<i class="fas fa-tags"></i>&nbsp;
            
                
                <a href="/tags/#notes">notes,</a>
                
            
                
                <a href="/tags/#math">math</a>
                
            
		</span-->
	</div>

	<!--table of content-->
	<div class="toc">
		<p class="toc-meta">Table of Contents</p>
		<div class="toc-content">
			
		</div>
	</div>
    
	<!--post content-->
    <article class="post-content">
        <h2 id="expectation-of-discrete-random-variables">Expectation of discrete random variables</h2>
<p>The intuition of expectation is the <em>average value</em> of an experiment. Suppose we do an experiment for $N$ repeated times. The probability of each possible outcome $x$ can be approximately defined by 
\begin{equation}
    \mathbf{P}(x) \approx f(x) = \frac{freq(x)}{N}.
\end{equation}
Then the average outcome is
\begin{equation}
    m \approx \frac{1}{N}\sum_{x} \text{freq}(x)x = \sum_x f(x)x.
\end{equation}</p>

<div class="theorem"><p><b>Definition 1.</b> 
The <strong>mean value</strong>, or <strong>expectation</strong>, or <strong>expected value</strong> of the random variable $X$ with mass function $f$ is defined to be 
$$\begin{equation}
    \mathbf{E}(X) = \sum_{x:f(x)&gt;0} xf(x)
\end{equation}$$
whenever this sum is absolutely convergent.
</p></div>

<div class="remark"><p><b>Remark.</b> 
<ol>
    <li>For notation convenience, we also write $\mathbf{E}(X) = \sum_x xf(x)$.</li>
    <li>We require **absolute convergence** in order that $\mathbf{E}(X)$ be unchanged by reordering the $x_i$.</li>
</ol>
</p></div>

<div class="theorem"><p><b>Theorem 1.</b><a href="https://en.wikipedia.org/wiki/Riemann_series_theorem"> Riemann Rearrangement Theorem</a>.
</p></div>

<div class="theorem"><p><b>Lemma.</b> 
If $X$ has mass function $f$ and $g:\mathbb{R}\to\mathbb{R}$, then
$$\begin{equation}
    \mathbf{E}(g(x)) = \sum_x g(x) f(x)
\end{equation}$$
whenever this sum is absolutely convergent.
</p></div>

<div class="example"><p><b>Example.</b> 
If $X$ is a random variable with mass function f, and $g(x) = x^2$, then 
$$\begin{equation}
    \mathbf{E}(X^2) = \sum_x g(x)f(x) = x^2 f(x).
\end{equation}$$
</p></div>

<div class="theorem"><p><b>Definition 2.</b> 
If $k$ is a positive integer, the $k$th <b>moment</b> $m_k$ of $X$ is defined to be 
\begin{equation}
    m_k = \mathbf{E}(X^k).
\end{equation}
The $k$th <b>central moment</b> $\sigma_k$ is defined as
\begin{equation}
    \sigma_k = \mathbf{E}\left( (X-m_1)^k \right) = \mathbf{E}\left( (X-E(X))^k \right).
\end{equation}
</p></div>

<p>The two moments of most use are $m_1 = \mathbf{E}(X)$ and $\sigma_2 = \mathbf{E}( (X - \mathbf{E}(X))^2$, called the <strong>mean</strong> (or <strong>expectation)</strong> and <strong>variance</strong> of $X$. These two quantities are measures of the mean and dispersion of $X$; that is, $m_1$ is the average value of $X$, and $\sigma_2$ measures the amount by which $X$ tends to deviate from this average. The mean $m_1$ is often denoted $\mu$, and the variance of $X$ is often denoted $\mathbf{Var}(X)$. The positive square root $\sigma = \mathbf{Var}(X)$ is called the <strong>standard deviation</strong>, and in this notation $\sigma_2 = \sigma^2$.</p>

<p>The central moments ${\sigma_i}$ can be expressed in terms of the ordinary moments ${m_i}$. For example, $\sigma_1 = 0$, and</p>

\[\begin{equation}
    \begin{aligned} 
        \sigma_{2} &amp;=\sum_{x}\left(x-m_{1}\right)^{2} f(x) \\ 
        &amp;=\sum_{x} x^{2} f(x)-2 m_{1} \sum_{x} x f(x)+m_{1}^{2} \sum_{x} f(x) \\ 
        &amp;=m_{2}-m_{1}^{2} ,
    \end{aligned}
\end{equation}\]

<p>which may be written as 
\(\begin{equation}
    \tag{4-1}
    \mathbf{Var}(X) = \mathbf{E}\left( (X-E(X))^2 \right) = \mathbf{E}(X^2) - \mathbf{E}(X)^2.
\end{equation}\)</p>

<div class="example"><p><b>Example.</b> [Binomial variables]
Let $X$ be a random variable with binomial distribution. The p.m.f. is 
$$\begin{equation}
    f(k) = \binom{n}{k} p^k q^{n-k} \quad k = 0,\dots, n,
\end{equation}$$
where $q = 1-p$. The expectation of $X$ is
$$\begin{equation}
    \mathbf{E}(X) = \sum_{k=0}^n k f(k) = \sum_{k=0}^n k\binom{n}{k} p^k q^{n-k}.
\end{equation}$$
We use the following algebraic identity to compute $\mathbf{E}(X)$.
$$\begin{equation}
    \label{eq:4.2}
    \tag{4-2}
    \sum_{k=0}^n \binom{n}{k} x^k = (1+x)^n, 
\end{equation}$$
Differentiate it and multiply by $x$, we obtain 
$$\begin{equation}
    \label{eq:4.3}
    \tag{4-3}
    \sum_{k=0}^n k \binom{n}{k} x^k = nx(1+x)^{n-1}. 
\end{equation}$$
We substitute $x = p / q$ to obtain $\mathbf{E}(X) = np$. A similar argument shows that the variance of $X$ is given by $\mathbf{Var}(X) = npq$. 
</p></div>

<p>We can think of the process of calculating expectations as a <strong>linear operator</strong> on the space of random variables.</p>

<div class="theorem"><p><b>Theorem 2.</b> 
The expectation operator $\mathbf{E}$ has the following properties: 
<ol>
    <li> if $X\geq 0$, then $\mathbf{E}(X) \geq 0$,</li>
    <li> if $a, b \in \mathbb{R}$, then $\mathbf{E}(aX+bY) = a\mathbf{E}(X) + b\mathbf{E}(Y)$,</li>
    <li> the random variable 1, taking the value 1 always, has expectation $\mathbf{E}(1) = 1$.</li>
</ol>
</p></div>

<div class="proof"><a href="javascript:void(0)" id="pf1-link"><b>Proof &#9656;</b></a>
<div class="proof-content" id="pf1-content"><p>
We only prove the second property, which is also called the linear property.

We must use the joint p.m.f. of $X$ and $Y$ to compute the expectation. 
$$\begin{equation}
    \begin{split}
        \mathbf{E}(aX+bY) &amp;= \sum_{i, j} (ax_i + by_j) f(x_i, y_j) \\
        &amp;= a \sum_{i,j} x_i f(x_i, y_j) + b\sum_{i,j} y_j f(x_i, y_j) \\
        &amp;= a\sum_{i} x_i f_X(x_i) + b\sum_{j}y_j f_Y(y_j) \\ 
        &amp;= a\mathbf{E}(X) + b\mathbf{E}(Y),
    \end{split}
\end{equation}$$
where $f_X(x)$ and $f_Y(y)$ are marginal p.m.f. of $X$ and $Y$ respectively.
&#9724;</p>
</div>
</div>
<script>toggle_proof("pf1");</script>

<div class="remark"><p><b>Remark.</b> 
It is <b>NOT</b> in general true that $\mathbf{E}(XY)$ is the same as $\mathbf{E}(X)\mathbf{E}(Y)$. 
</p></div>

<div class="theorem"><p><b>Lemma.</b> 
If $X$ and $Y$ are independent, then $\mathbf{E}(XY) = \mathbf{E}(X)\mathbf{E}(Y)$. 
</p></div>
<div class="proof"><a href="javascript:void(0)" id="pf2-link"><b>Proof &#9656;</b></a>
<div class="proof-content" id="pf2-content"><p>
If $X, Y$ are independent, $f(x,y) = f_X(x) f_Y(y)$. Then
$$\begin{equation}
    \mathbf{E}(XY) = \sum_{ij} x_i y_j f(x,y) = \sum_{i} \left( x_i f_X(x_i) \right) \sum_{j} \left( y_j f_Y(y_j) \right) = \mathbf{E}(X) \mathbf{E}(Y).
\end{equation}$$
\end{proof}
&#9724;</p>
</div>
</div>
<script>toggle_proof("pf2");</script>

<div class="theorem"><p><b>Definition 3.</b> 
$X$ and $Y$ are called **uncorrelated** if $\mathbf{E}(XY) = \mathbf{E}(X)\mathbf{E}(Y)$.
</p></div>

<div class="remark"><p><b>Remark.</b> 
Independent variables are uncorrelated. But the converse is **NOT** true.
</p></div>

<div class="theorem"><p><b>Theorem 3.</b> 
For random variables $X$ and $Y$, 
<ol>
    <li> $\mathbf{Var}(aX) = a^2 \mathbf{Var}(X)$ for $a \in \mathbb{R}$,</li>
    <li> $\mathbf{Var}(X+Y) = \mathbf{Var}(X) + \mathbf{Var}(Y)$ is $X$ and $Y$ are uncorrelated.</li>
</ol>
</p></div>

<div class="remark"><p><b>Remark.</b> 
The above theorem shows that the variance operator $\mathbf{Var}$ is **NOT** a linear operator, even when it 
is applied only to uncorrelated variables. 
</p></div>

<p>Sometimes the sum $S = \sum xf(x)$ does not converge absolutely, which means the mean of the distribution does not exist. Here is an example.</p>
<div class="example"><p><b>Example.</b> [A distribution without a mean] Let $X$ have mass function 
$$\begin{equation}
    f(k) = Ak^{-1} \quad k = \pm 1, \pm 2, \dots,
\end{equation}$$
where $A$ is chosen so that $\sum_k f(k) = 1$. The sum $\sum_k kf(k) = A\sum_{k\neq 0} k^{-1}$ doesn't converge absolutely, because both the positive and the negative parts diverge. 
</p></div>
<p>This example is suitable to point out that we can base probability theory upon the expectation operator $\mathbf{E}$ rather than upon the probability measure $\mathbf{P}$. Roughly speaking, the way we proceed is to postulate axioms, such as (a)-(c) of the above Theorem, for a so-called “expectation operator” $\mathbf{E}$ acting on a space of ``random variables”. The probability of an event can then be recaptured by defining $\mathbf{P}(A) = \mathbf{E}(I_A)$.</p>

<p>Recall the indicator function of a set $A$ is defined as
\(\begin{equation}
    I_A(\omega) = \begin{cases} 1 &amp; \omega \in A, \\
    0 &amp; \omega \not\in A. \end{cases}
\end{equation}\)
In addition, we have $\mathbf{E}(I_A) = \mathbf{P}(A)$.</p>

<h2 id="dependence-of-discrete-random-variables">Dependence of discrete random variables</h2>
<div class="theorem"><p><b>Definition 4.</b> 
The **joint distribution function** $F:\mathbb{R}^2 \to [0,1]$ of $X$ and $Y$, where $X$ and $Y$ are discrete variables, is given by 
$$\begin{equation}
    F(x, y) = \mathbf{P}(X\leq x \text{ and } Y \leq y). 
\end{equation}$$
Their **joint mass function** $f:\mathbb{R}^2 \to [0,1]$ is given by 
$$\begin{equation}
    f(x,y) = \mathbf{P}(X = x \text{ and } Y = y). 
\end{equation}$$
</p></div>

<p>We write $F_{X,Y}$ and $f_{X,Y}$ when we need to stress the role of $X$ and $Y$. We may think of the joint mass function in the following way. If $A_x = {X = x}$ and $B_y = {Y = y}$, then 
\(\begin{equation}
    f(x,y) = \mathbf{P}(A_x \cap B_y).
\end{equation}\)</p>

<div class="theorem"><p><b>Lemma.</b> 
The discrete random variables $X$ and $Y$ are **independent** if and only if 
$$\begin{equation}
    \tag{4-4}
    f_{X,Y}(x,y) = f_X(x)f_Y(y) \quad \forall x,y \in \mathbb{R}.
\end{equation}$$
More generally, $X$ and $Y$ are independent if and only if $f_{X,Y}(x, y)$ can be **factorized as the product** $g(x)h (y)$ of a function of $x$ alone and a function of $y$ alone. 
</p></div>
<div class="remark"><p><b>Remark.</b> 
We stress that the factorization Eq.(4-4) must hold for all $x$ and $y$ in order that $X$ and $Y$ be independent. 
</p></div>

<div class="theorem"><p><b>Lemma.</b> 
$$\begin{equation}
    \mathbf{E}(g(X, Y))=\sum_{x, y} g(x, y) f_{X, Y}(x, y).
\end{equation}$$
</p></div>

<div class="theorem"><p><b>Definition 5.</b> 
The covariance of $X$ and $Y$ is
$$\begin{equation}
    \mathbf{cov}(X,Y) = \mathbf{E}\left( (X-\mathbf{E}(X))(Y-\mathbf{E}(Y)) \right).
\end{equation}$$
The correlation (coefficient) of $X$ and $Y$ is 
$$\begin{equation}
\mathbf{corr}(X, Y) = \rho(X,Y) = \frac{\mathbf{cov}(X,Y)}{\sqrt{\mathbf{Var}(X)\mathbf{Var}(Y)}}
\end{equation}$$
as long as the variances are non-zero. 
</p></div>

<div class="remark"><p><b>Remark.</b> 
Notice the following two equations.
<ol>
    <li> $\mathbf{cov}(X,X) = \mathbf{Var}(X)$,</li>
    <li> $\mathbf{cov}(X,Y) = \mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.</li>
</ol>
</p></div>

<p>Covariance itself is not a satisfactory measure of dependence because the scale of values which $\mathbf{cov}(X, Y)$ may take contains no points which are clearly interpretable in terms of the relationship between $X$ and $Y$.</p>

<div class="theorem"><p><b>Theorem 4.</b> [Cauchy-Schwarz inequality] For random variables $X$ and $Y$, 
$$\begin{equation}
    \mathbf{E}(XY)^2 \leq \mathbf{E}(X^2) \mathbf{E}(Y^2)
\end{equation}$$ 
with equality if and only if $\mathbf{P}(aX = bY) = 1$ for some real $a$ and $b$, at least one of which is non-zero. 
</p></div>

<div class="proof"><a href="javascript:void(0)" id="pf3-link"><b>Proof &#9656;</b></a>
<div class="proof-content" id="pf3-content"><p>
For $a, b \in \mathbb{R}$, let $Z = aX - bY$. Then 
$$\begin{equation}
    0 \leq \mathbf{E}(Z^2) = a^2 \mathbf{E}(X^2) - 2ab\mathbf{E}(XY) + b^2\mathbf{E}(Y^2).
\end{equation}$$
Thus the right-hand side is a quadratic in the variable $a$ with at most one real root. Its discriminant must be non-positive. That is to say, if $b \neq 0$, 
$$\begin{equation}
    \mathbf{E}(XY)^2 - \mathbf{E}(X^2) \mathbf{E}(Y^2) \leq 0. 
\end{equation}$$
The discriminant is zero if and only if the quadratic has a real root. This occurs if and only if 
$$\begin{equation}
    \mathbf{E}\left( (aX-bY)^2 \right) = 0
\end{equation}$$
for some $a$ and $b$.
&#9724;</p>
</div>
</div>
<script>toggle_proof("pf3");</script>

<p>We define $X’ = X-\mathbf{E}(X), Y’ = Y - \mathbf{E}(Y)$. Since all $X, Y$ satisfy the Cauchy-Schwarz inequality, so do $X’$ and $Y’$. Therefore, 
\(\begin{equation}
    \mathbf{E}(X'Y')^2 \leq \mathbf{E}(X'^2) \mathbf{E}(Y'^2) \quad \Leftrightarrow \quad \mathbf{cov}(X, Y)^2 \leq \mathbf{Var}(X)\mathbf{Var}(Y).
\end{equation}\)
Therefore, 
\(\begin{equation}
    \rho(X,Y)^2 \leq 1 \quad \mathbb{R}ightarrow \quad \rho(X,Y) \in [-1, 1].
\end{equation}\)
which gives the following lemma.</p>

<div class="theorem"><p><b>Lemma.</b> 
The correlation coefficient $\rho$ satisfies $\left\vert \rho (X, Y)  \right\vert \leq 1$ with equality if and only if $\mathbf{P}(aX + bY = c) = 1$ for some $a, b, c \in \mathbb{R}$. 
</p></div>

<h2 id="expectation-of-continuous-random-variables">Expectation of continuous random variables</h2>
<h3 id="idea-of-translating-expectation-from-discrete-to-continuous">Idea of translating expectation from discrete to continuous</h3>
<p>Suppose we have a continuous random variable $X$ with $f$ being the probability density function. We split $X$ into small intervals $\Delta x$. Then $p_i = f(x_i)\Delta x$. $\frac{p_i}{\Delta x}$ is an approximation of probability density function. Therefore, 
\(\begin{equation}
    \mathbf{E}(X) \approx \sum_{i} x_i p_i = \sum_{i} x_i f(x_i) \Delta x,
\end{equation}\)
which is the Remann sum. We take the limit and get
\(\begin{equation}
    \mathbf{E}(x) = \int_{-\infty}^\infty x f(x)dx.
\end{equation}\)</p>

<h3 id="expectation">Expectation</h3>
<div class="theorem"><p><b>Definition 6.</b> 
The **expectation** of a continuous random variable $X$ with density function $f$ is given by 
$$\begin{equation}
    \mathbf{E}(X) = \int_{-\infty}^\infty xf(x) dx
\end{equation}$$
whenever this integral exists.
</p></div>

<div class="theorem"><p><b>Theorem 5.</b> 
If $X$ and $g(X)$ are continuous random variables, then $$\begin{equation}
    \mathbf{E}\left( g(X) \right) = \int_{-\infty}^\infty g(x)f(x) dx.
\end{equation}$$
</p></div>

<div class="theorem"><p><b>Definition 7.</b> 
The $k$th **moment** of a continuous variable $X$ is defined as
$$\begin{equation}
    \mathbf{E}(X^k) = \int_{-\infty}^\infty x^k f(x) dx
\end{equation}$$
whenever the integral converges.
</p></div>

<div class="example"><p><b>Example.</b> [Cauchy distribution] The random variable $X$ has the Cauchy distribution t if it has density 
function 
$$\begin{equation}
    f(x) = \frac{1}{\pi (1+x^2)}, \quad x \in \mathbb{R}.
\end{equation}$$
This distribution is notable for having no moments.
</p></div>

<h2 id="dependence-of-continuous-random-variables">Dependence of continuous random variables</h2>
<div class="theorem"><p><b>Definition 8.</b> 
The **joint distribution function** of $X$ and $Y$ is the function $F: \mathbb{R}^2 \to [0, 1]$ given by 
$$\begin{equation}
    F(x,y) = \mathbf{P}(X\leq x, Y \leq y).
\end{equation}$$
</p></div>

<div class="theorem"><p><b>Definition 9.</b> 
The random variables $X$ and $Y$ are **(jointly) continuous** with **joint (probability) density function** $f : \mathbb{R}^2 \to [0, \infty)$ if
$$\begin{equation}
    F(x, y)=\int_{v=-\infty}^{y} \int_{u=-\infty}^{x} f(u, v) d u d v \quad \text{for each } x, y\in\mathbb{R}. 
\end{equation}$$
If $F$ is sufficiently differentiable at the point $(x , y)$, then we usually specify 
$$\begin{equation}
    f(x, y)=\frac{\partial^{2}}{\partial x \partial y} F(x, y).
\end{equation}$$
</p></div>

<p><strong>Probabilities</strong>:</p>

\[\begin{equation}
    \begin{aligned} 
        \mathbf{P}(a \leq X \leq b, c \leq Y \leq d) &amp;=F(b, d)-F(a, d)-F(b, c)+F(a, c) \\ 
        &amp;=\int_{y=c}^{d} \int_{x=a}^{b} f(x, y) d x d y. \end{aligned}
\end{equation}\]

<p>If $B$ is a sufficiently nice subset of $\mathbb{R}^2$, then
\(\begin{equation}
    \mathbf{P} \left( (X, Y) \in B \right)=\iint_{B} f(x, y) d x d y.
\end{equation}\)</p>

<p><strong>Marginal distributions</strong>: The marginal distribution functions of $X$ and $Y$ are</p>

\[\begin{equation}
    F_{X}(x)=\mathbf{P}(X \leq x)=F(x, \infty), \quad F_{Y}(y)=\mathbf{P}(Y \leq y)=F(\infty, y). 
\end{equation}\]

\[\begin{equation}
    F_{X}(x)=\int_{-\infty}^{x}\left(\int_{-\infty}^{\infty} f(u, y) d y\right) d u.
\end{equation}\]

<p>Marginal density function of $X$ and $Y$:
\(\begin{equation}
    f_{X}(x)=\int_{-\infty}^{\infty} f(x, y) d y, \quad f_{Y}(y)=\int_{-\infty}^{\infty} f(x, y) d x.
\end{equation}\)</p>

<p><strong>Expectation</strong>: If $g: \mathbb{R}^2 \to \mathbb{R}$ is a sufficiently nice function, then</p>

\[\begin{equation}
    \mathbf{E}(g(X, Y))=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f(x, y) d x d y;
\end{equation}\]

<p>in particular, setting $g(x, y) = ax + by$,</p>

\[\begin{equation}
    \mathbf{E}(aX+bY) = a\mathbf{E}(X) + b\mathbf{E}(Y).
\end{equation}\]

<p><strong>Independence</strong>: The random variables $X$ and $Y$ are independent if and only if</p>

\[\begin{equation}
    F(x,y) = F_X(x) F_Y(y) \quad \forall x, y \in \mathbb{R},
\end{equation}\]

<p>which, for <strong>continuous random variables</strong>, is equivalent to requiring that</p>

\[\begin{equation}
    f(x,y) = f_X(x) f_Y(y).
\end{equation}\]

<div class="theorem"><p><b>Theorem 6.</b> [Cauchy-Schwarz inequality] For any pair $X, Y$ of jointly continuous variables, we have that 
$$\begin{equation}
    \mathbf{E}(XY)^2 \leq \mathbf{E}(X^2) \mathbf{E}(Y^2), 
\end{equation}$$
with equality if and only if $\mathbf{P}(aX = bY) = 1$ for some real $a$ and $b$, at least one of which is non-zero. 
</p></div>

    </article>

    <!--additional addons-->
    <hr style="height: 1px; margin: 1rem 0">
    <!--span class="level-item"><i class="far fa-clock"></i>&nbsp;8 minutes read (About 1210 words)</span-->
	<div class="post-meta" style="display: flex; justify-content: space-between; align-items: center;">
		<span>
			<i class="fas fa-tags"></i>&nbsp;
            
                
                <a href="/tags/notes">notes,</a>
                
            
                
                <a href="/tags/math">math</a>
                
            
		</span>
	</div>

</div>


<section class="post-pagination">
    
        <a href="/2020/10/notes-3.html">Previous: Continuous Randon Variables</a>
    

    
        <a href="/2020/10/notes-5.html">Next: LLN and conditional probability</a>
    
</section>
  

<a onclick="topFunction()" id="back-top-button"><i class="fa-solid fa-chevron-up fa-2x"></i></a>


<script>
let mybutton = document.getElementById("back-top-button");	// get the button
window.onscroll = function() {scrollFunction()};	// show the button when scrolls down 200px from the top

function scrollFunction() {
  	if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
    	mybutton.style.display = "block";
  	} 
	else {
    	mybutton.style.display = "none";
  	}
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  	document.body.scrollTop = 0;
  	document.documentElement.scrollTop = 0;
}
</script>


<script>
    var coll = document.getElementsByClassName("toc-meta");
    var i;
    
    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("toc-active");
        var content = this.nextElementSibling;  	// Note the collapsible content should right after the collapsible class.
        if (content.style.maxHeight){
          	content.style.maxHeight = null;
        } else {
        	content.style.maxHeight = content.scrollHeight + "px";
        } 
      });
    }
    </script>
        </div>
	</div>

    <footer class="footerbar">
    <div class="footerbar-container footerbar-content">
        &copy; 2024 <a href="/">HuskyDev</a> 
        <span style="padding-left:30px"></span>
        Powered by <a href="https://jekyllrb.com/">Jekyll</a>
    </div>
</footer>
</body>

</html>
