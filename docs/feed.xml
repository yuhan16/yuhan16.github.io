<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HuskyDev</title>
    <description>HuskDev is Yuhan&apos;s personal blog.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/zfeed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 03 Jul 2023 10:14:35 -0400</pubDate>
    <lastBuildDate>Mon, 03 Jul 2023 10:14:35 -0400</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>Introduction to Trajectory Optimization</title>
        <description>&lt;p&gt;I have summarized some mathematical background and common computational approaches in trajectory optimization.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;a href=&quot;/assets/pdf/Trajopt_intro.pdf&quot;&gt;TrajectOpt slides&lt;/a&gt;&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/images/blog/2023/2023-04-10-trajopt_intro_outline.png&quot; height=&quot;350&quot; /&gt;
    &lt;figcaption&gt;Fig. 1: Outline of trajectory optimization slides.&lt;/figcaption&gt;
&lt;/figure&gt;
</description>
        <pubDate>Mon, 10 Apr 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2023/04/intro-to-trajopt.html</link>
        <guid isPermaLink="true">http://localhost:4000/2023/04/intro-to-trajopt.html</guid>
        
        <category>optimization</category>
        
        <category>math</category>
        
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Introduction to Reinforcement Learning</title>
        <description>&lt;p&gt;I have been learning Reinforcement Learning (RL) recently and I want to share some slides as a personal summary on RL.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;a href=&quot;/assets/pdf/RL_intro.pdf&quot;&gt;RL slides&lt;/a&gt;&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/images/blog/2022/2022-06-10-RL_intro_outline.png&quot; height=&quot;350&quot; /&gt;
    &lt;figcaption&gt;Fig. 1: Outline of RL summary slides.&lt;/figcaption&gt;
&lt;/figure&gt;
</description>
        <pubDate>Fri, 10 Jun 2022 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/06/intro-to-rl.html</link>
        <guid isPermaLink="true">http://localhost:4000/2022/06/intro-to-rl.html</guid>
        
        <category>RL</category>
        
        <category>math</category>
        
        
        <category>Reinforcement Learning</category>
        
      </item>
    
      <item>
        <title>Random Walk</title>
        <description>&lt;h2 id=&quot;random-walk&quot;&gt;Random walk&lt;/h2&gt;
&lt;h3 id=&quot;basic-defitions&quot;&gt;Basic defitions&lt;/h3&gt;

&lt;p&gt;One of the simplest random processes is so-called “simple random walk”.&lt;/p&gt;

&lt;!--more--&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
Let $\{X_k\}_{k=1}^\infty$ be a sequence of i.i.d. discrete random variables with finite outcomes. For each positive integer $n$, we let $S_n$ denote the sum $X_1 +X_2 + \cdots + X_n$. The sequence $\{S_n\}_{n=1}^\infty$ is called a **random walk**.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
If the common range of the $X_k$’s is $\mathbb{R}^m$, then we say that $\{S_n\}$ is a random walk in $\mathbb{R}^m$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Sometimes we set $S_0 = a$ for some number $a$ as the **start value** of the random walk. This is common in the gambling analysis.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
The simple random walk has the Markov property ; that is
$$\begin{equation}
    \mathbf{P}\left(S_{m+n}=j | S_{0}, S_{1}, \ldots, S_{m}\right)=\mathbf{P}\left(S_{m+n}=j | S_{m}\right), \quad n \geq 0.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf1-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf1-content&quot;&gt;&lt;p&gt;
If one knows the value of $S_m$ , then the distribution of $S_{m+n}$ depends only on the jumps $X_{m+ 1} ,\dots, \\ X_{m+n}$, and cannot depend on further information concerning the values of $S_0, S_1, \dots , S_{m-1}$.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf1&quot;);&lt;/script&gt;

&lt;p&gt;The random walk has two main applications: gambling analysis and investment decision making. For example, in the gambling, we gamble at each step, $S_i$ is the total stack we have at step $i$, and we are interested in when there will be a &lt;strong&gt;return&lt;/strong&gt; or \text{equalization}, i.e., what is the probability that $S_0 = a = S_n$.&lt;/p&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; [Random walks on the real line]
We shall first consider the simplest non-trivial case of a random walk in $\mathbb{R}^1$, namely the case where the common distribution function of the random variables $X_n$ is given by
$$\begin{equation}
    \label{eq:9.1}
    \tag{9-1}
    f_{X}(x)=\left\{\begin{array}{ll}{1 / 2,} &amp;amp; {\text { if } x=\pm 1} \\ {0,} &amp;amp; {\text { otherwise }}\end{array}\right.
\end{equation}$$
We note that in this situation, all paths of length $n$ have the same probability, namely $2^{-n}$.

We can plot the random walk in the plane, where the horizontal axis represents time and the vertical axis represents the value of $S_n$. Given a sequence $\{S_n\}$ of partial sums, we first plot the points $(n, S_n)$, and then for each $k &amp;lt; n$, we connect $(k, S_k)$ and $(k + 1, S_{k+1})$ with a straight line segment. The **length** of a path is just the difference in the time values of the beginning and ending points on the path. This can be visualized in Figure \ref{fig:9.1}.

&lt;figure&gt;
    &lt;img src=&quot;/assets/images/blog/2020/2020-11-27-random_walk.png&quot; height=&quot;300&quot; /&gt;
    &lt;figcaption&gt;Fig.1: A random walk $S_n$.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;h3 id=&quot;returns-and-first-returns&quot;&gt;Returns and First Returns&lt;/h3&gt;
&lt;p&gt;This subsection studies the random walk in $\mathbb{R}^1$. The p.m.f. of $X_i$ is given by \eqref{eq:9.1}.&lt;/p&gt;

&lt;p&gt;We say that an &lt;strong&gt;equalization&lt;/strong&gt; has occurred, or there is a &lt;strong&gt;return&lt;/strong&gt; to the origin at time $n$, if $S_n = 0$. Note that the returns are possible &lt;strong&gt;only after even number of steps&lt;/strong&gt;. To calculate
the probability of an equalization at time $2m$, &lt;strong&gt;we need only count the number of paths of length $2m$ which begin and end at the origin&lt;/strong&gt;. The number of such paths is clearly
\(\begin{equation}
    \binom{2m}{m} = \frac{(2m)!}{m! m!}.
\end{equation}\)
Since each path has probability $2^{-2m}$, we have the following theorem.&lt;/p&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
The probability of a return to the origin at time $2m$ is given by
$$\begin{equation}
    u_{2 m}= \binom{2m}{m} 2^{-2 m}.
\end{equation}$$
The probability of a return to the origin at an odd time is 0.
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;A random walk is said to have a &lt;strong&gt;first return&lt;/strong&gt; to the origin at time $2m$ if $m &amp;gt; 0$, and $S_{2k} = 0$ for all $k &amp;lt; m$. We define $f_{2m}$ to be the probability of this event. (We also define $f_0 = 0$.) One can think
of the expression $f_{2m}2^{2m}$ as the number of paths of length $2m$ between the points $(0, 0)$ and $(2m, 0)$ that do not touch the horizontal axis except at the endpoints. We have the following theorem.&lt;/p&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
For $n \geq 1$, the probabilities $\{u_{2k}\}$ and $\{f_{2k}\}$ are related by the equation
$$\begin{equation}
    u_{2 n}=f_{0} u_{2 n}+f_{2} u_{2 n-2}+\cdots+f_{2 n} u_{0},
\end{equation}$$
where $f_0 = 0$ and $u_0 = 1$ as convention.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf2-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf2-content&quot;&gt;&lt;p&gt;
There are $u_{2n}2^{2n}$ paths of length $2n$ which have endpoints $(0, 0)$ and $(2n, 0)$. The collection of such paths can be partitioned into $n$ sets, depending upon the time of the first return to the origin. A path in this collection which has a first return to the origin at time $2k$ consists of an initial segment from $(0, 0)$ to $(2k, 0)$, in which no interior points are on the horizontal axis, and a terminal segment from $(2k, 0)$
to $(2n, 0)$, with no further restrictions on this segment. Thus, the number of paths in the collection which have a first return to the origin at time $2k$ is given by
$$\begin{equation}
    f_{2 k} 2^{2 k} u_{2 n-2 k} 2^{2 n-2 k}=f_{2 k} u_{2 n-2 k} 2^{2 n}.
\end{equation}$$
If we sum over $k$, we obtain the equation
$$\begin{equation}
    u_{2 n} 2^{2 n}=f_{0} u_{2 n} 2^{2 n}+f_{2} u_{2 n-2} 2^{2 n}+\cdots+f_{2 n} u_{0} 2^{2 n}
\end{equation}$$
Dividing both sides of this equation by $2^{2n}$ completes the proof.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf2&quot;);&lt;/script&gt;

&lt;p&gt;Using the generating functions, we have following theorem.&lt;/p&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
For $m \geq 1$, the probability of a first return to the origin at time
$2m$ is given by
$$\begin{equation}
    f_{2 m}=\frac{u_{2 m}}{2 m-1}=\frac{\binom{2m}{m}}{(2 m-1) 2^{2 m}}.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf3-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf3-content&quot;&gt;&lt;p&gt;
We begin by defining the generating functions
$$\begin{equation}
    U(x)=\sum_{m=0}^{\infty} u_{2 m} x^{m}
\end{equation}$$
and 
$$\begin{equation}
    F(x)=\sum_{m=0}^{\infty} f_{2 m} x^{m}.
\end{equation}$$
Note that two sequences converge when $\left\vert x \right\vert&amp;lt;1$. The preceding theorem says that 
$$\begin{equation}
    U(x)=1+U(x) F(x).
\end{equation}$$
(The presence of the $1$ on the right-hand side is due to the fact that $u_0$ is defined to be $1$, but the last theorem only holds for $m \geq 1$.) We can solve the above equation for $F(x)$, obtaining
$$\begin{equation}
    F(x)=\frac{U(x)-1}{U(x)}.
\end{equation}$$
Now, if we can find a closed-form expression for the function $U(x)$, we will also have a closed-form expression for $F(x)$. From the first theorem in this notes, we have
$$\begin{equation}
    U(x)=\sum_{m=0}^{\infty}\binom{2m}{m} 2^{-2 m} x^{m} = \sum_{m=0}^\infty \binom{2m}{m} \left( \frac{x}{4} \right)^m.
\end{equation}$$
From series expansion, we find that 
$$\begin{equation}
    \frac{1}{\sqrt{1-4 x}}=\sum_{m=0}^{\infty}\left(\begin{array}{c}{2 m} \\ {m}\end{array}\right) x^{m}.
\end{equation}$$
Therefore, we have 
$$\begin{equation}
    U(x)=\frac{1}{\sqrt{1-x}}, \quad F(x) = \frac{U(x)-1}{U(x)} = 1-\sqrt{1-x}.
\end{equation}$$
Although it is possible to compute the value of f2m using the Binomial Theorem, it is easier to note that $F&apos;(x) = U(x)/2$, so that the coefficients $f_{2m}$ can be found by integrating the series for $U(x)$. We obtain, for $m \geq 1$, 
$$\begin{equation}
    f_{2 m}=\frac{u_{2 m-2}}{2 m} = \frac{\binom{2m-2}{m-1}}{m 2^{2 m-1}} = \frac{\binom{2m}{m}}{(2 m-1) 2^{2 m}} = \frac{u_{2m}}{2m-1},
\end{equation}$$
since
$$\begin{equation}
    \binom{2m-2}{m-1} = \frac{m}{2(2 m-1)}\binom{2m}{m}.
\end{equation}$$
This completes the proof of the theorem.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf3&quot;);&lt;/script&gt;

&lt;h3 id=&quot;probability-of-eventual-return&quot;&gt;Probability of Eventual Return&lt;/h3&gt;
&lt;p&gt;In the symmetric random walk process in $R^m$, we first examine this question in the case that $m = 1$, and then we consider the general case.&lt;/p&gt;

&lt;p&gt;We should be careful when dealing with eventual return since the sample space seems to be the set of all walks of infinite length, and this set is non-denumerable. To avoid difficulties, we will define $w_n$ to be the probability that a first return has occurred no later than time $n$. Thus,
$w_n$ concerns the sample space of all walks of length $n$, which is a finite set. Then it is reasonable to define 
\(\begin{equation}
    w_{*}=\lim _{n \rightarrow \infty} w_{n}.
\end{equation}\)
This limit clearly exists and is at most one, since the sequence ${w_n}_{n=1}^\infty$ is an increasing sequence, and all of its terms are at most 1. In terms of the $f_n$ probabilities, we see that
\(\begin{equation}
    w_{2 n}=\sum_{i=1}^{n} f_{2 i}.
\end{equation}\)
Thus 
\(\begin{equation}
    w_{*}=\sum_{i=1}^{\infty} f_{2 i}.
\end{equation}\)
In previous proof we introduced the the generating function
\(\begin{equation}
    F(x)=\sum_{m=0}^{\infty} f_{2 m} x^{m} = 1-\sqrt{1-x}.
\end{equation}\)
which is convergent for $\left\vert x \right\vert &amp;lt; 1$. In fact, it also converges for $x = \pm 1$. Hence we see that 
\(\begin{equation}
    w_{*}=F(1)=1.
\end{equation}\)
Thus, with probability one, the particle returns to the origin in $\mathbb{R}^1$ random walk.&lt;/p&gt;

&lt;p&gt;Now we consider the eventual return in $\mathbb{R}^m$. We define $f_{2 n}^{(m)}$ to be the probability that the first return to the origin in $\mathbb{R}^m$ occurs at time $2n$. The quantity $u_{2 n}^{(m)}$ is defined in a similar manner. From the preceding theorem, we have 
\(\begin{equation}
    u_{2 n}^{(m)}=f_{0}^{(m)} u_{2 n}^{(m)}+f_{2}^{(m)} u_{2 n-2}^{(m)}+\cdots+f_{2 n}^{(m)} u_{0}^{(m)}.
\end{equation}\)
We continue to generalize previous work by defining
\(\begin{equation}
    U^{(m)}(x)=\sum_{n=0}^{\infty} u_{2 n}^{(m)} x^{n}, \quad F^{(m)}(x)=\sum_{n=0}^{\infty} f_{2 n}^{(m)} x^{n}.
\end{equation}\)
Then we see that 
\(\begin{equation}
    U^{(m)}(x)=1+U^{(m)}(x) F^{(m)}(x).
\end{equation}\)
These functions will always converge in the interval $(1, 1)$. In fact, since 
\(\begin{equation}
    w_{*}^{(m)}=\sum_{n=0}^{\infty} f_{2 n}^{(m)} \leq 1
\end{equation}\)
for all $m$, the series for $F^{(m)}(x)$ converges at $x = 1$ as well, and $F^{(m)}(x)$ is left continuous at $x = 1$, i.e.,
\(\begin{equation}
    \lim _{x \uparrow 1} F^{(m)}(x)=F^{(m)}(1).
\end{equation}\)
Thus, we have
\(\begin{equation}
    \label{eq:9.2}
    \tag{9-2}
    w_{*}^{(m)}=\lim _{x \uparrow 1} F^{(m)}(x)=\lim _{x \uparrow 1} \frac{U^{(m)}(x)-1}{U^{(m)}(x)},
\end{equation}\)
so to determine $w_{*}^{(m)}$, , it suffices to determine 
\(\begin{equation}
    \lim _{x \uparrow 1} U^{(m)}(x).
\end{equation}\)
We let $u^{(m)}$ denote this limit. Then we have 
\(\begin{equation}
    \label{eq:9.3}
    \tag{9-3}
    u^{(m)}=\sum_{n=0}^{\infty} u_{2 n}^{(m)}.
\end{equation}\)&lt;/p&gt;

&lt;p&gt;In $\mathbb{R}^2$ random walk, we have
\(\begin{equation}
    u_{2 n}^{(2)}=\frac{1}{4^{2 n}} \binom{2n}{n}^2. 
\end{equation}\)
Using Stirling’s Formula
\(\begin{equation}
    \label{eq:Stirling}
    \tag{Stirling}
    n! \sim n^n e^{-n} \sqrt{2\pi n} \quad \text{for  $n$  large.}
\end{equation}\)
we have
\(\begin{equation}
    \binom{2n}{n} \sim \frac{2^{2 n}}{\sqrt{\pi n}} \quad \mathbb{R}ightarrow \quad 
    u_{2 n}^{(2)} \sim \frac{1}{\pi n}.
\end{equation}\)
From this it follows easily that 
\(\begin{equation}
    \sum_{n=0}^{\infty} u_{2 n}^{(2)}
\end{equation}\)
diverges, so $w_{*}^{(2)}=1$, i.e., in $\mathbb{R}^2$, the probability of an eventual return is 1.&lt;/p&gt;

&lt;p&gt;In $\mathbb{R}^3$, we have 
\(\begin{equation}
    u_{2 n}^{(3)}=\frac{1}{2^{2 n}}\binom{2n}{n} \sum_{j, k}\left(\frac{1}{3^{n}} \frac{n !}{j ! k !(n-j-k) !}\right)^{2}, 
\end{equation}\)
Note that $\sum_{n=0}^{\infty} u_{2 n}^{(3)}$ converges, so $w_{*}^{(3)}$ is strictly less than one. This means that in $\mathbb{R}^3$, the probability of an eventual return to the origin is strictly less than one.&lt;/p&gt;

&lt;h2 id=&quot;gamblers-ruin&quot;&gt;Gambler’s Ruin&lt;/h2&gt;
&lt;p&gt;In this section, we remove the assumption that the random walk is symmetric. Instead, we assume that $p$ and $q$ are non-negative real numbers with $p + q = 1$, and that the common distribution function of the jumps of the random walk is
\(\begin{equation}
    f_{X}(x)=\left\{\begin{array}{ll}{p,} &amp;amp; {\text { if } x=1} \\ {q,} &amp;amp; {\text { if } x=-1}\end{array}\right.
\end{equation}\)&lt;/p&gt;

&lt;p&gt;We formulate the problem as follows.
A gambler starts with a “stake” of size $s$. She plays until her capital reaches the value $M$ or the value $0$. In the language of Markov chains, these two values correspond to absorbing states. We are interested in studying the probability of occurrence of each of these two outcomes.&lt;/p&gt;

&lt;p&gt;We begin by defining $S_k$ to be the probability that the gambler’s stake reaches 0, i.e., she is ruined, before it reaches $M$, given that the initial stake is $k$. We note that $S_0 = 1$ and $S_M = 0$. The &lt;strong&gt;fundamental relationship&lt;/strong&gt; among the $S_k$’s is the following:
\(\begin{equation}
    S_{k}= p S_{k+1} + q S_{k-1},
\end{equation}\)
where $1 \leq k \leq M-1$. This holds because if her stake equals $k$, and she plays one game, then her stake becomes $k + 1$ with probability $p$ and $k-1$ with probability $q$. Since $p+q = 1$, we can rewrite the equation as 
\(\begin{equation}
    p\left(S_{k+1}-S_{k}\right)=q\left(S_{k}-S_{k-1}\right) \quad \text{ or }  \quad 
    S_{k+1}-S_{k}=\frac{q}{p}\left(S_{k}-S_{k-1}\right).
\end{equation}\)
From this equation, it is easy to see that
\(\begin{equation}
    S_{k+1}-S_{k}=\left(\frac{q}{p}\right)^{k}\left(S_{1}-S_{0}\right). 
\end{equation}\)
We now use telescoping sums to obtain an equation in which the only unknown is $S_1$.
\begin{equation}
    \begin{split}
        -1 &amp;amp;= S_M - S_0 = \sum_{k=0}^{M-1}\left(S_{k+1}-S_{k}\right) \&lt;br /&gt;
        &amp;amp;= \sum_{k=0}^{M-1}\left(\frac{q}{p}\right)^{k}\left(S_{1}-S_{0}\right) = \left(S_{1}-S_{0}\right) \sum_{k=0}^{M-1}\left(\frac{q}{p}\right)^{k}.
    \end{split}
\end{equation}
Then we can solve for $S_j$, $1\leq j \leq M$:
\begin{equation}
    S_{j}=1-\frac{(q / p)^{j}-1}{(q / p)^{M}-1}.
\end{equation}&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt; Consider $M &amp;gt;&amp;gt; j &amp;gt;&amp;gt; 0$, $q/p &amp;gt;1 $, then $S_j = 1-(q/p)^{j-M} \sim 1$.&lt;/li&gt;
    &lt;li&gt; Consider $q/p &amp;lt; 1$, then $S_j = (q/p)^j \sim 0$.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 27 Nov 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/11/notes-9.html</link>
        <guid isPermaLink="true">http://localhost:4000/2020/11/notes-9.html</guid>
        
        <category>notes</category>
        
        <category>math</category>
        
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Markov Chain 2</title>
        <description>&lt;h2 id=&quot;irreducible-markov-chains-and-regular-markov-chains&quot;&gt;Irreducible Markov chains and regular Markov chains&lt;/h2&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
A Markov chain is called an **irreducible chain** if it is possible to go from every state to every state (not necessarily in one move).
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Note that absorbing chains are NOT irreducible chains.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
A Markov chain is called a **regular chain** if some power of the transition matrix has only positive elements. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Note that regular chains are irreducible chains. But the converse is NOT true as the following example shows.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
Let the transition matrix of a Markov chain be defined by
$$\begin{equation}
    P = \begin{bmatrix} 0 &amp;amp; 1 \\ 1 &amp;amp; 0 \end{bmatrix}.
\end{equation}$$
If we start from state 1, then next step we jump to state 2. If we start from state 2, then we jump to state 1. In other words, we are flipping between two states after each step. So the chain is irreducible. But 
$$\begin{equation}
    P^{2n} = \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; 1 \end{bmatrix}, \quad 
    P^{2n+1} = \begin{bmatrix} 0 &amp;amp; 1 \\ 1 &amp;amp; 0 \end{bmatrix}.
\end{equation}$$
So the chain is not regular.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Any transition matrix that has no zeros determines a regular Markov chain. However, it is possible for a regular Markov chain to have a transition matrix that has zeros.
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;We shall discuss two important theorems relating to regular chains. But before, we will use the following two lemmas.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
\label{lemma:8.1}
Suppose $X$ is a random variable with finite number of outcomes, with the greatest being $x_{max}$ and the least being $x_{min}$. Then we have 
$$\begin{equation}
    \mathbf{E}(X) \leq x_{max}, \quad \mathbf{E}(X) \geq x_{min}.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf1-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf1-content&quot;&gt;&lt;p&gt;
Assume $X$ has $n$ outcomes, and let $f(x_i) = \mathbf{P}(X = x_i)$, then we have
\begin{gather*}
    \mathbf{E}(X) = \sum_{i=1}^n x_i f(x_i) \geq \sum_{i=1}^n x_{min} f(x_i) = x_{min}, \\
    \mathbf{E}(X) = \sum_{i=1}^n x_i f(x_i) \leq \sum_{i=1}^n x_{max} f(x_i) = x_{max}.
\end{gather*}
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf1&quot;);&lt;/script&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
\label{lemma:8.2}
Suppose a transition matrix $P$ only has positive entries and the smallest entry is $d&amp;gt;0$. Consider a vector $y$. Denote its maximum value as $M_0$ and its minimum value as $m_0$. Now consider the product $Py$. Denote its maximum value as $M_1$ and its minimum value as $m_1$. Then we have
$$\begin{equation}
    M_1 - m_1 \leq (1-2d) (M_0 - m_0).
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf2-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf2-content&quot;&gt;&lt;p&gt;
Note that $P$ is a transition matrix. So each row of $P$ sums up to $1$. From lemma \ref{lemma:8.1}, we note that each entry in the vector $Py$ is a weighted average of the entries in $y$, therefore, we have
$$\begin{equation}
    \label{eq:8.1}
    \tag{8-1}
    M_1 \leq M_0, \quad m_1 \geq m_0.
\end{equation}$$
The largest weighted average that could be obtained in the present case would occur if all but one of the entries of $y$ have value $M_0$ and one entry has value $m_0$, and this one small entry is weightedby the smallest possible weight, namely $d$. In this case, the weighted average would equal $m_0 d + M_0(1-d)$.
%$$\begin{equation}
%    m_0 d + M_0(1-d).
%\end{equation}$$
Similarly, the smallest possible weighted average equals $M_0 d + m_0(1-d)$.
%$$\begin{equation}
%    M_0 d + m_0(1-d).
%\end{equation}$$
Therefore, we have 
%Since \eqref{eq:8.1} is true for any $y$, we can take some particular form of $y$ to find the best and worst outcome of $Py$. First let $y = \begin{bmatrix} M_0 &amp;amp; \cdots &amp;amp; m_0 &amp;amp; \cdots &amp;amp; M_0 \end{bmatrix}$ such that $y$ only one entry being $m_0$ and such entry captures the minimum entry $d$ of $P$. Then we can show that 
$$\begin{equation}
    \label{eq:8.2}
    \tag{8-2}
    M_1 \leq m_0 d + M_0(1-d),
\end{equation}$$
%Similarly, by letting $y = \begin{bmatrix} m_0 &amp;amp; \cdots &amp;amp; M_0 &amp;amp; \cdots &amp;amp; m_0 \end{bmatrix}$ such that $y$ only one entry being $M_0$ and such entry captures the minimum entry $d$ of $P$, we can obtain 
$$\begin{equation}
    \label{eq:8.3}
    \tag{8-3}
    m_1 \geq M_0 d + m_0(1-d). 
\end{equation}$$
\eqref{eq:8.2}-\eqref{eq:8.3} yields 
$$\begin{equation}
    M_1 - m_1 \leq (1-2d) (M_0 - m_0).
\end{equation}$$
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf2&quot;);&lt;/script&gt;

&lt;p&gt;Now we give two important theorems.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; [Fundamental limit theorem for regular chains]
\label{thm:8.1}
Let $P$ be the transition matrix for a regular chain. Then, as $n \to \infty$, the powers $P^n$ approach a limiting matrix $W$ with all rows the **same** vector $w$. The vector $w$ is a strictly positive probability vector (i.e.., the components are all **positive** and they sum to one).
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf3-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf3-content&quot;&gt;&lt;p&gt;
We consider an arbitrary vector $y$, and denote the maximum entry of $P^n y$ as $M_n$ and the minimum entry of $P^n y$ as $m_n$. From lemma 1, we must have 
$$\begin{equation}
    M_n \leq M_{n-1}, \quad m_n \geq m_{n+1}.
\end{equation}$$
Writing all these inequality from 1 to $n$ gives
$$\begin{equation}
    M_0 \geq M_1 \geq \cdots \geq M_n \geq m_n \geq \cdots \geq m_1 \geq m_0.
\end{equation}$$
Therefore $\{M_n\}$ is a decreasing sequence bounded below and $\{m_n\}$ is an increasing sequence bounded above. From sequence theory we know that $\{ M_n \}$ and $\{m_n\}$ must converge. Let
$$\begin{equation}
    M = \lim_{n\to\infty} M_n, \quad m = \lim_{n\to\infty} m_n.
\end{equation}$$
From Lemma \ref{lemma:8.2} we have 
$$$$$$$$$$\begin{equation}
    \label{eq:8.4}
    \tag{8-4}
    M_n - m_n \leq (1-2d)^n (M_0 - m_0).
\end{equation}$$$$$$$$$$
On the other hand, since $d$ is the minimum entry of $P$, we must have $0 &amp;lt; d \leq \frac{1}{2}$, which gives $0 \leq 1-2d &amp;lt; 1$. Thus from \eqref{eq:8.4} we have 
$$$$$$$$$$\begin{equation}
    \label{eq:8.5}
    \tag{8-5}
    \lim_{n\to\infty} (M_n - m_n) = 0 \quad \Rightarrow \quad 
    M=m \quad \forall \ y.
\end{equation}$$$$$$$$$$
We choose a particular form of $y$ such that $y = e_j$ with $j$th entry being 1 and others being 0. Then $P^n y$ gives the $j$th column of $P^n$. Using \eqref{eq:8.5}, we have
$$\begin{equation}
    \lim_{n\to\infty} P^n y = \alpha \mathbf{1} \quad \text{for some $\alpha &amp;gt; 0$},
\end{equation}$$
which shows the $j$th column has the same entries.

Thus, by letting $y = e_j$, $1\leq j \leq n$, we conclude that each column of $P^n$ has the same entries. This finishes the proof.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf3&quot;);&lt;/script&gt;

&lt;p&gt;The $ij$th entry of $P^n$, $p^{(n)}_{ij}$, is the probability that the process will be in state $s_j$ after $n$ steps if it starts in state $s_i$. If we denote the common row of $W$ by $w$, then Theorem \ref{thm:8.1} states that the probability of being in $s_j$ in the long run is approximately $w_j$, the $j$th entry of $w$, and is independent of the starting state.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
\label{thm:8.2}
Let $P$ be a regular transition matrix, let
$$\begin{equation}
    W = \lim_{n\to\infty} P^n,
\end{equation}$$
and let $w$ be the common row of $W$. Then
&lt;ol&gt;
    &lt;li&gt; 
    $wP = w$, which means $w$ is a left eigenvector of $P$ with eigenvalue 1, and any row vector $v$ such that $vP = v$ is a constant multiple of $w$.&lt;/li&gt;
    &lt;li&gt; 
    $P\mathbf{1} = \mathbf{1}$, which means $\mathbf{1}$ is an eigenvector of $P$ with eigenvalue 1, and any column vector $x$ such that $Px = x$ is a multiple of $\mathbf{1}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf4-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf4-content&quot;&gt;&lt;p&gt;
To prove part (a), we note that from Theorem \ref{thm:8.1}, 
$$\begin{equation}
    \lim_{n\to\infty} P^n = W.
\end{equation}$$
Thus
$$\begin{equation}
    \lim_{n\to\infty} P^{n+1} = P^n P = W P.
\end{equation}$$
But $\lim_{n\to\infty} P^{n+1} = W$, and so $W = WP$ and $w = wP$.

Let $v$ be any vector with $vP = v$. Then $v = vP^n$, and passing to the limit, $v = vW$. Let $r$ be the sum of the components of $v$, i.e., $r = \sum_{i=1}^n v_i$. Then it is easily checked that $vW = rw$. So, $v = rw$.

To prove part (b), assume that $x = Px$. Then $x = P^n x$, and again passing to the limit, $x = Wx$. Since all rows of $W$ are the same, the components of $Wx$ are all equal, so $x$ is a multiple of $\mathbf{1}$.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf4&quot;);&lt;/script&gt;

&lt;p&gt;Note that an immediate consequence of Theorem \ref{thm:8.2} is the fact that there is only one &lt;strong&gt;probability vector&lt;/strong&gt; $v$ such that $vP = v$, which is $v = w$. The probability vector $v$ is the vector such that $\sum_{i=1}^n = 1$.&lt;/p&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Computing $W$ is equivalent to computing the left eigenvector of $P$ with eigenvalue 1. It is equivalent to compute the right eigenvector of $P^T$ with eigenvalue 1.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
Let $P$ be a transition matrix such that 
$$\begin{equation}
    P = \begin{bmatrix}  \frac{1}{2} &amp;amp; \frac{1}{2} \\ \frac{1}{4} &amp;amp; \frac{3}{4} \end{bmatrix}.
\end{equation}$$
Compute $W = \lim_{n\to\infty} P^n$. 

This is equivalent to compute $P^T x = x$. Note that $x$ is a probability vector, so $\mathbf{1}^T x = 1$.
$$\begin{equation}
    P^T x = x \quad \Rightarrow \quad \begin{cases} \frac{1}{2} x_1 + \frac{1}{4}x_2 = x_1 \\ x_1+x_2 = 1 \end{cases} \quad \Rightarrow \quad
    \begin{cases} x_1 = \frac{1}{3} \\ x_2 = \frac{2}{3} \end{cases}
\end{equation}$$
Therefore $w = [\frac{1}{3} \ \frac{2}{3}]$, and $W = \begin{bmatrix} w \\ w \end{bmatrix} = \begin{bmatrix} \frac{1}{3} &amp;amp; \frac{2}{3} \\ \frac{1}{3} &amp;amp; \frac{2}{3} \end{bmatrix}$.
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;fixed-vectors&quot;&gt;Fixed vectors&lt;/h2&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
A row vector $w$ with the property $wP = w$ is called a **fixed row vector** for $P$. Similarly, a column vector $x$ such that $Px = x$ is called a **fixed column vector** for $P$.
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Thus, the common row of $W$ is the unique vector $w$ which is both a fixed row vector for $P$ and a probability vector. Theorem \ref{thm:8.2} shows that any fixed row vector for $P$ is a multiple of $w$ and any fixed column vector for $P$ is a constant vector. One can also state the above definition in terms of eigenvalues and eigenvectors. A fixed row vector is a left eigenvector of the matrix $P$ corresponding to the eigenvalue 1. A similar statement can be made about fixed column vectors.&lt;/p&gt;

&lt;p&gt;The following theorem generalizes Theorem \ref{thm:8.1} to the case where the starting state is itself
determined by a probability vector.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 8.3.&lt;/b&gt; 
Let $P$ be the transition matrix for a regular chain and $v$ an arbitrary probability vector. Then
$$\begin{equation}
    \lim_{n\to\infty} vP^n = w,
\end{equation}$$
where $w$ is the unique fixed probability vector for $P$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf5-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf5-content&quot;&gt;&lt;p&gt;
By Theorem \ref{thm:8.1},
$$\begin{equation}
    \lim_{n\to\infty} P^n = W.
\end{equation}$$
Hence 
$$\begin{equation}
    \lim_{n\to\infty} vP^n = vW.
\end{equation}$$
But the entries in $v$ sum to 1, and each row of $W$ equals $w$. From these statements, it is easy to check that
$$\begin{equation}
    vW = w.
\end{equation}$$
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf5&quot;);&lt;/script&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
If we start a Markov chain with initial probabilities given by $v$, then the probability vector $v P^n$ gives the probabilities of being in the various states after $n$ steps. Theorem \ref{thm:8.3} then establishes the fact that, even in this more general class of processes, the probability of being in $s_j$ approaches $w_j$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Theorem 8.3 tells that regular chains always equilibriate to $w$.
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Does the result holds for irreducible chains?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOT&lt;/strong&gt; true for irreducible chains. The following is a counterexample.&lt;/p&gt;
&lt;div class=&quot;Example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
Let 
$$\begin{equation}
    P = \begin{bmatrix} 0 &amp;amp; 1 \\ 1 &amp;amp; 0 \end{bmatrix}.
\end{equation}$$
If we calculate the left eigenvector, we can obtain 
$$\begin{equation}
    w = \begin{bmatrix} \frac{1}{2} &amp;amp; \frac{1}{2} \end{bmatrix}.
\end{equation}$$
But if we start from $(1,0)$ or $(0,1)$, we will never goes to this equilibrium.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
The period $d(i)$ of a state $i$ is defined by $d(i) = \gcd\{n : P_{ii}(n) &amp;gt; 0\}$, the greatest common divisor of the epochs at which return is possible. We call state $i$ periodic if $d(i) &amp;gt; 1$ and aperiodic if $d(i) = 1$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Aperiodic + irreducible $\Leftrightarrow$ regular for finite outcome Markov chains.
&lt;/p&gt;&lt;/div&gt;

</description>
        <pubDate>Fri, 20 Nov 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/11/notes-8.html</link>
        <guid isPermaLink="true">http://localhost:4000/2020/11/notes-8.html</guid>
        
        <category>notes</category>
        
        <category>math</category>
        
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Markov Chain</title>
        <description>&lt;p&gt;Most of our study of probability has dealt with independent trials processes, in which we have i.i.d. random variables $X_i$. This is useful for modeling and sampling situations. But when study random dynamical systems, they are less useful. In these cases, ${X_i}$ are no longer independent because they are all part of the system, and they have to interact with each other. So one way to modeling this is to use conditional probability.&lt;/p&gt;

&lt;h2 id=&quot;markov-chains-basic-definitions&quot;&gt;Markov chains: basic definitions&lt;/h2&gt;
&lt;p&gt;Consider a countable sequence $X_1, X_2, \dots, X_n, \dots$ which&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt; the subscript represents time, thus we have discrete time sequence;&lt;/li&gt;
    &lt;li&gt; each $X_i$ has finite outcomes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, each $X_i$ is a discrete random variable that takes one of $N$ possible values, where 
$N = \mathbf{card}(S)$ and $S$ is the outcome space; it may be the case that $N = \infty$.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
The process $X$ is a Markov chain if it satisfies the &lt;b&gt;Markov condition&lt;/b&gt;:
$$\begin{equation}
    \mathbf{P}\left(X_{n}=s | X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{n-1}=x_{n-1}\right)=\mathbf{P}\left(X_{n}=s | X_{n-1}=x_{n-1}\right)
\end{equation}$$
for all $n\geq 1$ ans all $s, x_1, \dots, x_{n-1} \in S$.
&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;Since $S$ is assumed countable, it can be put in one-to-one correspondence with some subset $S’$ of the integers, and without loss of generality we can assume that $S$ is this set $S’$ of integers. Then the evolution of a chain is described by its &lt;strong&gt;transition probabilities&lt;/strong&gt; 
\(\begin{equation}
    \label{eq:7.1}
    \tag{7-1}
    \mathbf{P}(X_{n} = j \;\vert\; X_{n-1} = i) := p_{ij}(n-1).
\end{equation}\)&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
The transition matrix $P$ is a **stochastic matrix**, which is to say that: 
&lt;ol&gt;
    &lt;li&gt; $P$ has non-negative entries, or $p_{ij} \geq 0$ for all $i, j$,&lt;/li&gt;
    &lt;li&gt; $P$ has row sums equal to one, or $\sum_j p_{ij} = 1$ for all $i$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;%Note that the Markov property is equivalent to the following stipulation, which is also called &lt;strong&gt;$k$-step transition probabilities&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
The **$k$-step transition probabilities** is defined as
$$\begin{equation}
    \label{eq:7.2}
    \tag{7-2}
    \mathbf{P}(X_{n+k-1} = j \;\vert\; X_{n-1} = i) := p_{ij}^{(k)}(n-1) \quad \text{for any } n, k \geq 1.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;%Based on this, we can define&lt;/p&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
The **transition matrix** $P(n-1) = (p_{ij}(n-1))$ is the $\left\vert S \right\vert \times \left\vert S \right\vert$ matrix of **transition probabilities** $p_{ij}(n-1)$. The **$k$-step transition matrix** $P^{(k)}(n-1) = (p_{ij}^{(k)}(n-1))$ is the matrix of **$k$-step transition probabilities** $p_{ij}^{(k)}(n-1)$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
We can check that the Markov property is equivalent to each of the following two stipulations: for each $s \in S$ and for every sequence $\{ x_i \;\vert\; i \geq 0\}$ in $S$,
$$\begin{equation}
    \label{eq:7.3a}
    \tag{7-3a}
    \begin{split}
        \mathbf{P}\left(X_{n+1}=s | X_{n_{1}}=x_{n_{1}}, X_{n_{2}}=x_{n_{2}}, X_{n_{k}}=x_{n_{k}}\right)=\mathbf{P}\left(X_{n+1}=s | X_{n_{k}}=x_{n_{k}}\right) \\
        \text{for all } n_1 &amp;lt; n_2 &amp;lt; \cdots &amp;lt; n_k \leq n.
    \end{split}
\end{equation}$$
$$\begin{equation}
    \label{eq:7.3b}
    \tag{7-3b}
    \begin{split}
        \mathbf{P}\left(X_{m+n}=s | X_{0}=x_{0}, X_{1}=x_{1}, \ldots, X_{m}=x_{m}\right)=\mathbf{P}\left(X_{m+n}=s | X_{m}=x_{m}\right) \\ 
        \text{for any } m, n \geq 0.
    \end{split}
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Next we introduce another assumption:&lt;/p&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
The chain $X$ is called **homogeneous** if
$$\begin{equation}
    \mathbf{P}(X_n = j \;\vert\; X_{n-1} = i) = \mathbf{P}(X_2 = j \;\vert\; X_1 = i), \quad \text{for all } n, i, j.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Note the following two remarks.
&lt;ol&gt;
    &lt;li&gt; Markov property and homogeneity property are **independent**. There are also Markov chains which are not homogeneous.&lt;/li&gt;
    &lt;li&gt; If a Markov chain $X$ is homogeneous, then $p_{ij}(n-1)$ doesn&apos;t depends on the time $(n-1)$, i.e., the probability transition matrix $P$ is **fixed** at each step.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;properties-of-markov-chains&quot;&gt;Properties of Markov chains&lt;/h2&gt;
&lt;p&gt;By the assumption of homogeneity, we know $P(n-1) = P$. That $P^{(k)}(n-1)$ doesn’t depend on $(n-1)$ is a consequence of the following important fact.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; [Chapman-Kolmogorov equations]
$$\begin{equation}
    p_{ij}^{(n+r)}(m) = \sum_{k} p_{ik}^{(n)}(m) p_{kj}^{(r)}(m+n).
\end{equation}$$
Therefore, 
$$\begin{equation}
    P^{(n+r)}(m) = P^{(n)}(m) P^{(r)}(m+n).
\end{equation}$$
Furthermore, if we have homogeneity, then 
$$\begin{equation}
    P^{n}(m) = P^n,
\end{equation}$$
the $n$th power of $P$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf1-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf1-content&quot;&gt;&lt;p&gt;
    $$\begin{equation}
        \begin{split}
            p_{ij}^{n+r}(m) &amp;amp;= \mathbf{P}(X_{m+n+r} = j \;\vert\; X_m = i) \\ 
            &amp;amp;= \sum_{k} \mathbf{P}(X_{m+n+r} = j, X_{m+n} = k \;\vert\; X_m = i) \\
            &amp;amp;= \sum_{k} \mathbf{P}(X_{m+n+r} = j \;\vert\; X_{m+n} = k, X_m = i) \mathbf{P}(X_{m+n} = k \;\vert\; X_m = i) \quad \text{(marginalization)} \\
            &amp;amp;= \sum_{k} \mathbf{P}(X_{m+n+r} = j \;\vert\; X_{m+n} = k) \mathbf{P}(X_{m+n} = k \;\vert\; X_m = i) \quad \text{(Markov property)} \\
            &amp;amp;= \sum_{k} p_{kj}^{(r)}(m+n) p_{ik}^{(n)}(m) . 
        \end{split}
    \end{equation}$$
    The marginalization step uses the fact that 
    $$\begin{equation}
        \mathbf{P}(A \cap B \;\vert\; C) = \mathbf{P}(A \;\vert\; B \cap C) \mathbf{P}(B \;\vert\; C).
    \end{equation}$$
    The Markov property step uses \eqref{eq:7.3a}. The $n$th power is obtained by iteration.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf1&quot;);&lt;/script&gt;

&lt;p&gt;One consequence of the preceding theorem is that $P^{(n)}(m) = P^{(n)}(0)$. Note that this consequence is obtained &lt;strong&gt;with homogeneous assumption&lt;/strong&gt;. We write $P_n$ for $P^{(n)}(m)$ and $p_{ij}(n)$ for $p_{ij}^{(n)}(m)$. This theorem relates long-term development to short-term development, and tells us how $X_n$ depends on the initial variable $X_0$. Let $\mu_i^{(n)} = \mathbf{P}(X_n = i)$ be the mass function of $X_n$, and write $\mu^{(n)}$ for the &lt;strong&gt;row vector&lt;/strong&gt; with entries $(\mu_i^{(n)} : i \in S)$. Then we have the following lemma.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
$\mu^{(m+n)} = \mu^{(m)}P_n$, and hence $\mu^{(n)} = \mu^{(0)}P^n$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf2-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf2-content&quot;&gt;&lt;p&gt;
    We have that
    $$\begin{equation}
        \begin{split}
            \mu_j^{(m+n)} &amp;amp;= \mathbf{P}(X_{m+n} = j) \\
            &amp;amp;= \sum_{i} \mathbf{P}(X_{m+n} = j \;\vert\; X_m = i) \mathbf{P}(X_m = i) \\
            &amp;amp;= \sum_{i} \mu_i^{(m)} p_{ij}(n) \\
            &amp;amp;= (\mu^{(m)} P_n)_j
        \end{split}
    \end{equation}$$
    and the result follows from the preceding theorem.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf2&quot;);&lt;/script&gt;

&lt;p&gt;The random evolution of the chain is determined by the transition matrix $P$ and the initial mass function $\mu^{(0)}$.&lt;/p&gt;

&lt;h2 id=&quot;absorbing-markov-chains&quot;&gt;Absorbing Markov chains&lt;/h2&gt;
&lt;p&gt;An absorbing Markov chain is a special type of Markov chains.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
A state $s_i$ of a Markov chain is called **absorbing** if it is impossible to leave it (i.e., $p_{ii} = 1$). A Markov chain is **absorbing** if it has at least one absorbing state, and if from every state it is possible to go to an absorbing state (not necessarily in one step).
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
In an absorbing Markov chain, a state which is not absorbing is called **transient**.
&lt;/p&gt;&lt;/div&gt;

&lt;h3 id=&quot;canonical-form-of-absorbing-markov-chains&quot;&gt;Canonical form of absorbing Markov chains&lt;/h3&gt;
&lt;p&gt;Consider an arbitrary absorbing Markov chain. Renumber the states so that the transient states come first. If there are $r$ absorbing states and $t$ transient states, the transition matrix will have the following canonical form
\begin{equation}
    P = 
    \begin{blockarray}{rcc}
        &amp;amp; TR. &amp;amp; ABS. &lt;br /&gt;
    \begin{block}{r[cc]}
        TR. &amp;amp; Q &amp;amp;  R &lt;br /&gt;
        ABS. &amp;amp; 0 &amp;amp;  I &lt;br /&gt;
    \end{block}
    \end{blockarray}
\end{equation}
Here $I$ is an $r\times r$ identity matrix, $0$ is an $r\times t$ zero matrix, $R$ is a nonzero $t\times r$ matrix, and $Q$ is an $t\times t$ matrix. The first $t$ states are transient and the last $r$ states are absorbing.&lt;/p&gt;

&lt;p&gt;A standard matrix algebra argument shows that $P^n$ is of the form
\begin{equation}
    P^n = 
    \begin{blockarray}{rcc}
        &amp;amp; TR. &amp;amp; ABS. &lt;br /&gt;
    \begin{block}{r[cc]}
        TR. &amp;amp; Q^n &amp;amp;  * &lt;br /&gt;
        ABS. &amp;amp; 0 &amp;amp;  I &lt;br /&gt;
    \end{block}
    \end{blockarray}
\end{equation}
where the asterisk $*$ stands for the $t\times r$ matrix in the upper right-hand corner of $P^n$.&lt;/p&gt;

&lt;p&gt;The entries of $Q_n$ give the probabilities for being in each of the transient states after $n$ steps for each possible transient starting state. The following theorem will show that every entry of $Q_n$ will approach zero as $n$ approaches infinity (i.e., $Q_n \to 0$).&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
In an absorbing Markov chain, the probability that the process will be absorbed is 1 (i.e., $Q_n \to 0$ as $n \to \infty$).
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf3-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf3-content&quot;&gt;&lt;p&gt;
From each non-absorbing state $s_j$ it is possible to reach an absorbing state. Let $m_j$ be the minimum number of steps required to reach an absorbing state, starting from $s_j$. Let $p_j$ be the probability that, starting from $s_j$, the process will not reach an absorbing state in $m_j$ steps. Then $p_j &amp;lt; 1$. Let $m$ be the largest of the $m_j$ and let $p$ be the largest of $p_j$. The probability of not being absorbed in $m$ steps is less than or equal to $p$, in $2m$ steps less than or equal to $p^2$, etc. Since $p &amp;lt; 1$, these probabilities tend to 0. Since the probability of not being absorbed in $n$ steps is monotone decreasing, these probabilities also tend to 0, hence $\lim_{n\to\infty} Q_n = 0$.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf3&quot;);&lt;/script&gt;

&lt;h3 id=&quot;the-fundamental-matrix&quot;&gt;The fundamental matrix&lt;/h3&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
For an absorbing Markov chain $P$, the matrix $N = (I-Q)^{-1}$ is called the **fundamental matrix** for $P$. The entry $n_{ij}$ of $N$ gives the expected number of times that the process is in the transient state $s_j$ if it is started in the transient state $s_i$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
For an absorbing Markov chain the matrix $I-Q$ has an inverse $N$ and $N = I + Q + Q^2 + \cdots$. The $ij$-entry $n_{ij}$ of the matrix $N$ is the expected number of times the chain is in state $s_j$, given that it starts in state $s_i$. The initial state is counted if $i = j$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf4-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf4-content&quot;&gt;&lt;p&gt;
Let $(I-Q)x = 0$; that is $x = Qx$. Then, iterating this we see that $x = Q^n x$. Since $Q_n \to 0$, we have $Q_n x \to 0$, so $x = 0$. Thus $(I-Q)^{-1} = N$
exists. Note next that
$$\begin{equation}
    (I-Q)(I+Q + Q^2 + \cdots + Q^n) = I - Q^{n+1}.
\end{equation}$$
Thus multiplying both sides by $N$ gives
$$\begin{equation}
    I + Q + Q^2 + \cdots + Q^n = N(I-Q^{n+1}) .
\end{equation}$$
Letting $n$ tend to infinity we have
$$\begin{equation}
    N = I + Q + Q^2 + \cdots .
\end{equation}$$
Let $s_i$ and $s_j$ be two transient states, and assume throughout the remainder of the proof that $i$ and $j$ are fixed. Let $X^{(k)}$ be a random variable which equals 1 if the chain is in state $s_j$ after $k$ steps, and equals 0 otherwise. For each $k$, this random variable depends upon both $i$ and $j$; we choose not to explicitly show this dependence in the interest of clarity. We have
$$\begin{equation}
    \mathbf{P} \left(X^{(k)}=1\right)=q_{i j}^{(k)},
\end{equation}$$
and 
$$\begin{equation}
    \mathbf{P} \left(X^{(k)}=0\right)=1-q_{i j}^{(k)},
\end{equation}$$
where $q_{i j}^{(k)}$ is the $ij$th entry of $Q^k$. These equations hold for $k = 0$ since $Q^0 = I$. Therefore, since $X^{(k)}$ is a 0-1 random variable, $\mathbf{E}(X^{(k)}) = q_{i j}^{(k)}$.

The expected number of times the chain is in state $s_j$ in the first $n$ steps, given that it starts in state $s_i$, is clearly
$$\begin{equation}
    \mathbf{E}\left(X^{(0)}+X^{(1)}+\cdots+X^{(n)}\right)=q_{i j}^{(0)}+q_{i j}^{(1)}+\cdots+q_{i j}^{(n)}.
\end{equation}$$
Letting $n$ tend to infinity we have
$$\begin{equation}
    \mathbf{E}\left(X^{(0)}+X^{(1)}+\cdots\right)=q_{i j}^{(0)}+q_{i j}^{(1)}+\cdots=n_{i j}.
\end{equation}$$
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf4&quot;);&lt;/script&gt;

&lt;h3 id=&quot;time-to-absorption&quot;&gt;Time to absorption&lt;/h3&gt;
&lt;p&gt;Given that the chain starts in state $s_i$, what is the expected number of steps before the chain is absorbed? The following theorem gives the answer.&lt;/p&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
Let $t_i$ be the expected number of steps before the chain is absorbed, given that the chain starts in state $s_i$, and let $t$ be the **column vector** whose $i$th entry is $t_i$. Then
$$\begin{equation}
    t = N \mathbf{1}
\end{equation}$$
where $\mathbf{1}$ is a column vector all of whose entries are 1.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf5-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf5-content&quot;&gt;&lt;p&gt;
If we add all the entries in the $i$th row of $N$, we will have the expected number of times in any of the transient states for a given starting state $s_i$, that is, the expected time required before being absorbed. Thus, $t_i$ is the sum of the entries in the $i$th row of $N$. If we write this statement in matrix form, we obtain the theorem.
\end{proof}&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf5&quot;);&lt;/script&gt;

&lt;h3 id=&quot;absorption-probabilities&quot;&gt;Absorption probabilities&lt;/h3&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
Let $b_{ij}$ be the probability that an absorbing chain will be absorbed in the absorbing state $s_j$ if it starts in the transient state $s_i$. Let $B$ be the matrix with entries $b_{ij}$ . Then $B$ is an $t\times r$ matrix, and
$$\begin{equation}
    B = NR,
\end{equation}$$
where $N$ is the fundamental matrix and $R$ is as in the canonical form.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf6-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf6-content&quot;&gt;&lt;p&gt;
We have 
$$\begin{equation}
    \begin{aligned} 
        B_{i j} &amp;amp;=\sum_{n} \sum_{k} q_{i k}^{(n)} r_{k j} \\ &amp;amp;=\sum_{k} \sum_{n} q_{i k}^{(n)} r_{k j} \\ &amp;amp;=\sum_{k} n_{i k} r_{k j} \\ &amp;amp;=(N R)_{i j}.
    \end{aligned}
\end{equation}$$
This completes the proof.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf6&quot;);&lt;/script&gt;

</description>
        <pubDate>Fri, 13 Nov 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/11/notes-7.html</link>
        <guid isPermaLink="true">http://localhost:4000/2020/11/notes-7.html</guid>
        
        <category>notes</category>
        
        <category>math</category>
        
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Generating Functions</title>
        <description>&lt;h2 id=&quot;generating-functions&quot;&gt;Generating functions&lt;/h2&gt;
&lt;p&gt;A sequence $a = {a_i \;\vert\; i = 0, 1 , 2, \dots }$ of real numbers may contain a lot of information. One concise way of storing this information is to wrap up the numbers together in a ``generating function”. For example, the (ordinary) &lt;strong&gt;generating function&lt;/strong&gt; of the sequence $a$ is the function $G_a$ defined by 
\(\begin{equation}
    G_{a}(s)=\sum_{i=0}^{\infty} a_{i} s^{i} \quad \text { for } s \in \mathbb{R} \text { for which the sum converges. }
\end{equation}\)
In many circumstances it is easier to work with the generating function $G_a$ than with the original sequence $a$.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; [Abel&apos;s theorem]
If $a_i \geq 0$ for all $i$ and $G_a(s)$ is finite for $\left\vert s \right\vert &amp;lt; 1$, then $\lim_{s \uparrow 1} G_a (s) = \sum_{i=1}^ \infty a_i$, whether the sum is finite or equals $+\infty$. This standard result is useful when the radius of convergence $R$ satisfies $R = 1$, since then one has no a priori right to take the limit as $s \uparrow 1$.
&lt;/p&gt;&lt;/div&gt;

&lt;h3 id=&quot;moment-generating-function&quot;&gt;Moment generating function&lt;/h3&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
The **moment generating function** of the random variable $X$ is the function $M: \mathbb{R} \mapsto [0, \infty)$ given by the Laplace transform of the corresponding p.d.f. $f_X(s)$:
$$\begin{equation}
    M_X(t) = \mathbf{E} \left( e^{tX} \right) = \int_{-\infty}^\infty e^{tx} f_X(x) dx,
\end{equation}$$
or corresponding p.m.f. $p_X(k)$:
$$\begin{equation}
    M_X(t) = \sum_{k} e^{t k} \mathbf{P}(X=k) 
    = \sum_k \sum_{n=0}^{\infty} \frac{(t k)^{n}}{n !} \mathbf{P}(X=k)
    = \sum_{n=0}^{\infty} \frac{t^{n}}{n !}\left(\sum_k k^{n} \mathbf{P}(X=k)\right) 
    = \sum_{n=0}^{\infty} \frac{t^{n}}{n !} \mathbf{E}\left(X^{n}\right).
\end{equation}$$
$M_X(-t)$ is so called **bilateral** Laplace transform of $f_X(x)$ or $p_X(k)$.
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Under the assumption that $M_X(t)$ is infinitely differentiable at $t=0$, the following statements are true.&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt; $M^\prime(0) = \mathbf{E}(0) = \mu$.&lt;/li&gt;
    &lt;li&gt; $M^{(n)}(0) = \mathbf{E}(X^n)$.&lt;/li&gt;
    &lt;li&gt; Using Taylor&apos;s theorem, $M_X(t) = \sum_{k=0}^\infty \frac{t^k}{k!} \mathbf{E}(X^k)$.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
If $X$ and $Y$ are independent, then
$$\begin{equation}
    \begin{split}
        M_{X+Y}(t) &amp;amp;= \int_{-\infty}^\infty e^{tz} f_{x+y}(z) dz \\
        &amp;amp;= \int_{-\infty}^\infty e^{tz} \int_{-\infty}^\infty f_X(x) f_Y(z-x) dx dz \\
        &amp;amp;= \int_{-\infty}^\infty e^{t(x+y)} \int_{-\infty}^\infty f_X(x) f_Y(y) dx dy \\
        &amp;amp;= M_X(t) M_Y(t).
    \end{split}
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;h3 id=&quot;characteristic-functions&quot;&gt;Characteristic functions&lt;/h3&gt;
&lt;p&gt;Sometimes $\mathbf{E}(e^{tX})$ may blow up. So we consider some transformations in the complex domain, which usually perform better.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
The **characteristic function** of $X$ is the function $\phi: \mathbb{R} \mapsto \mathbb{C}$ defined by
$$\begin{equation}
    \phi(t)=\mathbf{E}\left(e^{i t X}\right) \quad \text { where } \quad i=\sqrt{-1}.
\end{equation}$$
We often write $\phi_x$ for the characteristic function of the random variable $X$. Characteristic functions are related to Fourier transforms. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
The characteristic function $\phi$ satisfies:
&lt;ol&gt;
    &lt;li&gt; $\phi(0) = 1$, $\left\vert \phi(t) \right\vert \leq 1$ for all $t$.&lt;/li&gt;
    &lt;li&gt; $\phi$ is uniformly continuous on $\mathbb{R}$ w.r.t. $t$.&lt;/li&gt;
    &lt;li&gt; If $X \sim \mathcal{N}(0,1)$, then $\phi_{X}(t) = e^{-t^2/2}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf1-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf1-content&quot;&gt;&lt;p&gt;
We only prove the first statement.
$$\begin{equation}
    \begin{split}
        \left\vert \phi(t) \right\vert &amp;amp;= \left\vert \int_{-\infty}^\infty e^{itx} f(x)dx  \right\vert \\
        &amp;amp;\leq \int_{-\infty}^\infty \left\vert e^{itx} \right\vert f(x) dx \quad \text{(triangle inequality)} \\
        &amp;amp;= \int_{-\infty}^\infty f(x) dx \quad (\left\vert e^{itx} \right\vert=1) \\
        &amp;amp;= 1
    \end{split}
\end{equation}$$
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf1&quot;);&lt;/script&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; [Cauchy distribution]
If $f(x) = 
\frac{1}{\pi(1+x^2)}$, then the corresponding characteristic function is 
$$\begin{equation}
    \phi(t)=\frac{1}{\pi} \int_{-\infty}^{\infty} \frac{e^{i t x}}{1+x^{2}} d x = e^{-\left\vert t \right\vert}.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
The following statements are true.
&lt;ol&gt;
    &lt;li&gt; If $\phi^{(k)}(0)$ exists, then 
    $$\begin{equation}
        \left\{\begin{array}{ll}{\mathbf{E}\left|X^{k}\right|&amp;lt;\infty} &amp;amp; {\text { if $k$}  \text { is even }} \\ {\mathbf{E}\left|X^{k-1}\right|&amp;lt;\infty} &amp;amp; {\text { if $k$ } \text { is odd }}\end{array}\right.
    \end{equation}$$&lt;/li&gt;
    &lt;li&gt; If $\mathbf{E}(\left\vert X^k \right\vert) &amp;lt; \infty$, then 
    $$\begin{equation}
        \phi(t)=\sum_{j=0}^{k} \frac{\mathbb{E}\left(X^{j}\right)}{j !}(i t)^{j}+\mathrm{o}\left(t^{k}\right)
    \end{equation}$$
    and so $\phi^{(k)}(0)=i^{k} \mathbb{E}\left(X^{k}\right)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
If $X$ and $Y$ are independent then 
$$\begin{equation}
    \phi_{X+Y}(t)=\phi_{X}(t) \phi_{Y}(t).
\end{equation}$$
Similarly, if $X_1, \dots, X_n$ are independent, then 
$$\begin{equation}
    \phi_{X_1 + \cdots + X_n}(t)= \prod_{i=1}^n \phi_{X_i}(t).
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
If $a, b \in \mathbb{R}$ and $Y = aX+b$, then $\phi_{Y}(t)=e^{i t b} \phi_{X}(a t)$.
&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf2-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf2-content&quot;&gt;&lt;p&gt;
$$\begin{equation}
    \phi_{Y}(t)=\mathbf{E}\left(e^{i t(a X+b)}\right)=\mathbf{E}\left(e^{i t b} e^{i(a t) X}\right) = e^{i t b} \mathbf{E}\left(e^{i(a t) X}\right)=e^{i t b} \phi_{X}(a t).
\end{equation}$$
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf2&quot;);&lt;/script&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
Random variables $X$ and $Y$ are independent if and only if
$$\begin{equation}
    \phi_{X, Y}(s, t)=\phi_{X}(s) \phi_{Y}(t) \quad \text { for all } s \text { and } t.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
We say that the sequence $F_1 , F_2, \dots $ of distribution functions converges to the distribution function $F$, written $F_n \to F$, if $F(x) = \lim_{n\to\infty} F_n(x)$ at each point $x$ where $F$ is continuous.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; [Continnity theorem]
Suppose that $F_1 , F_2, \dots $ is a sequence of distribution functions 
with corresponding characteristic functions $\phi_1 , \phi_2, \dots $.
&lt;ol&gt;
    &lt;li&gt; If $F_n \to F$ for some distribution function $F$ with characteristic function $\phi$, then $\phi_n(t) \to \phi(t)$ for all $t$.&lt;/li&gt;
    &lt;li&gt; Conversely, if $\phi(t) \lim_{n\to\infty} \phi_n(t)$ exists and is continuous at $t=0$, then $\phi$ is the characteristic function of some distribution function $F$, and $F_n \to F$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;central-limit-theorem&quot;&gt;Central limit theorem&lt;/h2&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
If $X, X_1 , X_2 , \dots$ is a sequence of random variables with respective distribution functions $F, F_1, F_2, \cdots$, we say that $X_n$ converges in distribution to $X$, written $X_{n} \stackrel{\mathrm{D}}{\rightarrow} X$, if $F_n \to F$ as $n \to\infty$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; [Central limit theorem]
Let $X_1 , X_2, \dots$ be a sequence of independent identically distributed random variables with finite mean $\mu$ and finite nonzero variance $\sigma^2$, and let $S_n = X_1 + X_2 + \cdots + X_n$. Then
$$\begin{equation}
    \frac{S_{n}-n \mu}{\sqrt{n \sigma^{2}}} \stackrel{\mathrm{D}}{\rightarrow} N(0,1) \quad \text { as } \quad n \rightarrow \infty.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf3-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf3-content&quot;&gt;&lt;p&gt;
First, write $Y_i = \frac{X_i - \mu}{\sigma}$, and let $\phi_Y$ be the characteristic function of the $Y_i$. We have that 
$$\begin{equation}
    \phi_{Y}(t)=1-\frac{1}{2} t^{2}+o\left(t^{2}\right).
\end{equation}$$
Note that $Y_i$ are i.i.d. So the characteristic function of $\sum_{i=1}^n Y_i$ is
$$\begin{equation}
    \phi_n = [\phi_{Y}(t)]^n = \left[ 1-\frac{1}{2} t^{2}+o\left(t^{2}\right) \right]^n.
\end{equation}$$
Also, the characteristic function $\psi_n$ of
$$\begin{equation}
    U_{n}=\frac{S_{n}-n \mu}{\sqrt{n \sigma^{2}}}=\frac{1}{\sqrt{n}} \sum_{i=1}^{n} Y_{i}
\end{equation}$$
satisfies
$$\begin{equation}
    \psi_{n}(t)=\left\{\phi_{Y}(t / \sqrt{n})\right\}^{n}=\left\{1-\frac{t^{2}}{2 n}+o\left(\frac{t^{2}}{n}\right)\right\}^{n} \rightarrow e^{-\frac{1}{2} t^{2}} \quad \text { as } \quad  n \rightarrow \infty,
\end{equation}$$
where we used 
$$\begin{equation}
    \lim_{n\to\infty} \left( 1 + \frac{a}{n} \right)^n = e^a.
\end{equation}$$
The last function is the characteristic function of the $\mathcal{N}(0, 1)$ distribution, and an application of the continuity theorem completes the proof.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf3&quot;);&lt;/script&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Corollary.&lt;/b&gt;
$Q_n = \frac{1}{n}S_n \to \mathcal{N} \left(\mu, \frac{\sigma^2}{n} \right)$, $S_n \to \mathcal{N}(\mu, n \sigma^2)$. The sampling error is proportional to $\frac{1}{\sqrt{n}}$.
&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;There is a generalization. If $X_i$ is not i.i.d., we can still use the central limit theorem.&lt;/p&gt;
</description>
        <pubDate>Sat, 07 Nov 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/11/notes-6-1.html</link>
        <guid isPermaLink="true">http://localhost:4000/2020/11/notes-6-1.html</guid>
        
        <category>notes</category>
        
        <category>math</category>
        
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Sum of Random Variables</title>
        <description>&lt;p&gt;This lecture we study the expectation of the average of $n$ i.i.d. random variables $X_i$, $i=1,\dots, n$, i.e., the expectation of $Q_n = \frac{1}{n} (X_1 + \dots + X_n)$. As $n \to \infty$, we will introduce the central limit theorem and show that $Q_n$ converges to a normal distribution provided $\mathbf{Var}(X_i)$ exists.&lt;/p&gt;

&lt;h2 id=&quot;sums-of-discrete-random-variables&quot;&gt;Sums of Discrete Random Variables&lt;/h2&gt;
&lt;p&gt;Suppose $X$ and $Y$ are two independent discrete random variables with distribution functions $p_X(x)$ and $p_Y(y)$. Let $Z = X + Y$, we want to find the the distribution function of $Z$.&lt;/p&gt;

&lt;p&gt;Suppose that $X = k$, where $k$ is some integer. Then $Z = z$ if and only if $Y = z-k$. So the event $Z = z$ is the union of the pairwise disjoint events
\(\begin{equation}
    (X=k) \quad  \text { and } \quad  (Y=z-k)
\end{equation}\)
where $k$ runs over the integers. Since these events are pairwise disjoint, we have
\(\begin{equation}
    \label{eq:6.1}
    \tag{6-1}
    \mathbf{P}(Z=z)=\sum_{k=-\infty}^{\infty} \mathbf{P}(X=k)  \mathbf{P}(Y=z-k)
\end{equation}\)
which is the &lt;strong&gt;distribution function of the random variable $Z$&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
Let $X$ and $Y$ be two independent integer-valued random variables, with distribution functions $p_X(x)$ and $p_Y(y)$ respectively. Then the **convolution** of $p_X(x)$ and $p_Y(y)$ is the distribution function $p_Z = p_X * p_Y$ given by
$$\begin{equation}
    p_Z(z) = \sum_{x} p_X(x) p_Y(z-x)
\end{equation}$$
for $x\in \mathbb{Z}$. The function $p_Z(z)$ is the distribution function of the random variable $Z = X + Y$.
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;sums-of-continuous-random-variables&quot;&gt;Sums of Continuous Random Variables&lt;/h2&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition.&lt;/b&gt; 
Let $X$ and $Y$ be two continuous random variables with density functions $f_X(x)$ and $f_Y(y)$ respectively. Assume that both $f_X(x)$ and $f_Y(y)$ are defined for all real numbers. Then the **convolution** $f * g$ of $f$ and $g$ is the function given by
$$\begin{equation}
    \label{eq:6.2}
    \tag{6-2}
    (f_X * f_Y)(z) = \int_{-\infty}^{+\infty} f_X(z-y) f_Y(y) d y = \int_{-\infty}^{+\infty} f_Y(z-x) f_X(x) d x.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
Let $X$ and $Y$ be two independent random variables with density functions $f_X(x)$ and $f_Y(y)$ defined for all $x$ and $y$. Then the sum $Z = X + Y$ is a random variable with density function $f_Z(z)$, where $f_Z$ is the convolution of $f_X$ and $f_Y$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem.&lt;/b&gt; 
Let $\{X_i\}$, $i=1,\dots, n$ be a sequence of independent random variables with density functions $f_{X_1}(x), \dots, f_{X_n}(x)$ respectively, then we have 
$$\begin{equation}
    f_{X_1 + \cdots + X_n}(x) = f_{X_1} * \left( f_{X_2} * ( \cdots * f_{X_n}) \right)(x).
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; [Sum of two independent uniform random variables]
Let $X$ and $Y$ be random variables describing our choices and $Z = X + Y$ their sum. Then we have
$$\begin{equation}
    f_{X}(x)=f_{Y}(x)=\left\{\begin{array}{ll}{1} &amp;amp; {\text { if } 0 \leq x \leq 1} \\ {0} &amp;amp; {\text { otherwise }}\end{array}\right.
\end{equation}$$
and the density function for the sum is given by
$$\begin{equation}
    f_{Z}(z)=\int_{-\infty}^{+\infty} f_{X}(z-y) f_{Y}(y) d y = \int_{0}^{1} f_{X}(z-y) d y.
\end{equation}$$
Now the integrand is $0$ unless $0 \leq z-y \leq 1$ and then it is $1$. So if $0 \leq z \leq 1$, we have 
$$\begin{equation}
    f_{Z}(z)=\int_{0}^{z} d y=z,
\end{equation}$$
while if $1 &amp;lt; z \leq 2$, we have
$$\begin{equation}
    f_{Z}(z)=\int_{z-1}^{1} d y=2-z, 
\end{equation}$$
and if $z &amp;lt; 0$ or $z &amp;gt; 2$ we have $f_Z(z) = 0$. Hence
$$\begin{equation}
    f_{Z}(z)=\left\{\begin{array}{ll}{z,} &amp;amp; {\text { if } 0 \leq z \leq 1} \\ {2-z,} &amp;amp; {\text { if } 1&amp;lt;z \leq 2} \\ {0,} &amp;amp; {\text { otherwise }}\end{array}\right.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; [Sum of two independent exponential random variables]
Let $X, Y$, and $Z = X + Y$ denote the relevant random variables, and $f_X$ , $f_Y$ , and $f_Z$ their densities. Then
$$\begin{equation}
    f_{X}(x)=f_{Y}(x)=\left\{\begin{array}{ll}{\lambda e^{-\lambda x},} &amp;amp; {\text { if } x \geq 0} \\ {0,} &amp;amp; {\text { otherwise }}\end{array}\right.
\end{equation}$$
If $z &amp;gt; 0$,
$$\begin{equation}
    f_{Z}(z)=\int_{-\infty}^{+\infty} f_{X}(z-y) f_{Y}(y) d y =\int_{0}^{z} \lambda e^{-\lambda(z-y)} \lambda e^{-\lambda y} d y =\int_{0}^{z} \lambda^{2} e^{-\lambda z} d y =\lambda^{2} z e^{-\lambda z},
\end{equation}$$
while if $z &amp;lt; 0$, $f_Z(z) = 0$. Hence 
$$\begin{equation}
    f_{Z}(z)=\left\{\begin{array}{ll}{\lambda^{2} z e^{-\lambda z},} &amp;amp; {\text { if } z \geq 0} \\ {0,} &amp;amp; {\text { otherwise }}\end{array}\right.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

</description>
        <pubDate>Fri, 06 Nov 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/11/notes-6.html</link>
        <guid isPermaLink="true">http://localhost:4000/2020/11/notes-6.html</guid>
        
        <category>notes</category>
        
        <category>math</category>
        
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>LLN and conditional probability</title>
        <description>&lt;h2 id=&quot;law-of-large-numbers&quot;&gt;Law of large numbers&lt;/h2&gt;
&lt;p&gt;Note that in section, we are dealing with random variables with &lt;strong&gt;independent, identical distribution&lt;/strong&gt;, also written as &lt;strong&gt;i.i.d.&lt;/strong&gt; The law of large numbers aims to study the convergence of the average sum of large &lt;strong&gt;i.i.d.&lt;/strong&gt; random variables.&lt;/p&gt;

&lt;p&gt;We first prove the following important lemma.&lt;/p&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; [Chebyshev Inequality]
Let $X$ be a random variable with $\mathbf{E}(X) &amp;lt; \infty$ and $\mathbf{Var}(X) &amp;lt; \infty$, then for any $\epsilon &amp;gt; 0$, we have
$$\begin{equation}
    \label{eq:5.1}
    \tag{5-1}
    \mathbf{P}\left( \left\vert X-\mathbf{E}(X) \right\vert \geq \epsilon \right) \leq \frac{\mathbf{Var}(X)}{\epsilon^2}.
\end{equation}$$
In other words, we have
$$\begin{equation}
    \label{eq:5.2}
    \tag{5-2}
    \mathbf{P}(\left\vert X-\mathbf{E}(X) \right\vert &amp;lt; \epsilon) \geq 1-\frac{\mathbf{Var}(X)}{\epsilon^2}.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf1-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf1-content&quot;&gt;&lt;p&gt;
We assume $X$ is a discrete random variable. It can be easily extend to the case where $X$ is continuous. We denote $\mathbf{E}(X) = \mu$ and $f(x)$ as the p.m.f. of $X$.

We first expand the LHS of \eqref{eq:5.1} and obtain
$$\begin{equation}
    \mathbf{P}(\left\vert X-\mu \right\vert \geq \epsilon)  = \sum_{\left\vert x-\mu \right\vert\geq \epsilon} f(x).
\end{equation}$$
On the other hand, we have
$$\begin{equation}
    \begin{split}
        \mathbf{Var}(X) &amp;amp;= \sum_{x} (x-\mu)^2 f(x) \\
        &amp;amp;\geq \sum_{\left\vert x-\mu \right\vert \geq \epsilon} (x-\mu)^2 f(x) \\
        &amp;amp;\geq \sum_{\left\vert x-\mu \right\vert \geq \epsilon} \epsilon^2 f(x) \\
        &amp;amp;= \epsilon^2 \sum_{\left\vert x-\mu \right\vert \geq \epsilon} f(x) \\ &amp;amp;= \epsilon^2 \mathbf{P}(\left\vert X-\mu \right\vert \geq \epsilon).
    \end{split}
\end{equation}$$
Therefore, we have 
$$\begin{equation}
    \mathbf{P}(\left\vert X-\mu \right\vert \geq \epsilon) \leq \frac{\mathbf{Var}(X)}{\epsilon^2}.
\end{equation}$$
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf1&quot;);&lt;/script&gt;

&lt;p&gt;Chebyshev’s Inequality is the best possible inequality in the sense that, for any $\epsilon &amp;gt; 0$, it is possible to give an example of a random variable for which Chebyshev’s Inequality is in fact an equality.&lt;/p&gt;
&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
Suppose we have a random variable $X$ such that for any $\epsilon &amp;gt; 0$, $f(-\epsilon) = f(\epsilon) = \frac{1}{2}$. Clearly, $\mathbf{E}(X) = 0 &amp;lt; \infty$ and $\mathbf{Var}(X) = \mathbf{E}(X^2) = \epsilon^2 &amp;lt; \infty$. Therefore,
$$\begin{equation}
    \frac{\mathbf{Var}(X)}{\epsilon^2} = 1.
\end{equation}$$
Also note that $\mathbf{P}(\left\vert X-\mu \right\vert \geq \epsilon) = 1$. The equality sign of Chebyshev inequality holds. We cannot get better result.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 1.&lt;/b&gt; [Law of large numbers]
Consider a sequence of i.i.d. random variables $X_i$ with finite mean and variance. Denote $\mathbf{E}(X) = \mu$ and $\mathbf{Var}(X) = \sigma^2$. Define 
$$\begin{equation}
    Q_n = \frac{1}{n}\left( X_1 + X_2 + \cdots + X_n \right),
\end{equation}$$
then for any $\epsilon &amp;gt; 0$, 
$$\begin{equation}
    \lim_{n\to\infty} \mathbf{P}(\left\vert Q_n - \mu \right\vert \geq \epsilon ) = 0,
\end{equation}$$
or 
$$\begin{equation}
    \lim_{n\to\infty} \mathbf{P}(\left\vert Q_n - \mu \right\vert &amp;lt; \epsilon) = 1.
\end{equation}$$
This means $Q_n$ converges to $\mu$ in probability.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf2-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf2-content&quot;&gt;&lt;p&gt;
We notice that 
$$\begin{equation}
    \mathbf{E}(Q_n) = \sum_{i=1}^n \mathbf{E}\left( \frac{1}{n} X_i \right) = \frac{1}{n} \sum_{i=1}^n \mathbf{E}(X_i) = \frac{1}{n} n\mu = \mu,
\end{equation}$$
which shows that the expectation of $Q_n$ is the same as the expectation of $X_i$. We also have
$$\begin{equation}
    \mathbf{Var}(Q_n) = \mathbf{Var} \left( \frac{1}{n} \left(X_1 + \cdots + X_n \right) \right) = \frac{1}{n^2} \sum_{i=1}^n \mathbf{Var}(X_i) = \frac{\sigma^2}{n}.
\end{equation}$$
Using Chebyshev inequality, for any $\epsilon &amp;gt; 0$, we have
$$\begin{equation}
    \mathbf{P}(\left\vert Q_n-\mu \right\vert\geq \epsilon) \leq \frac{\mathbf{Var}(Q_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2}.
\end{equation}$$
Therfore,
$$\begin{equation}
    \lim_{n\to\infty} \mathbf{P}(\left\vert Q_n-\mu \right\vert\geq \epsilon) \leq \lim_{n\to\infty} \frac{\sigma^2}{n\epsilon^2} = 0.
\end{equation}$$
Since the probability is nonnegative, we must have
$$\begin{equation}
    \lim_{n\to\infty} \mathbf{P}(\left\vert Q_n-\mu \right\vert\geq \epsilon) = 0.
\end{equation}$$
This finishes the proof.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf2&quot;);&lt;/script&gt;

&lt;p&gt;This result is significant from the view of frequentist statistics. Recall the probability of an event $A$ is motivated by $\mathbf{P}(A) \approx N(A) / N$ where $N(A)$ and $N$ the number of occurrence of $A$ and the number of total experiments respectively. Now we can let $X_i = \mathbf{1}_A$, which is the indicator of the event $A$. Since each experiment is independent, we are actually perform a series Bernoulli trails and $X_i$ is the simple Bernoulli variable. Then we can write $N(A) = X_1 + \cdots + X_n$. Now 
\(\begin{equation}
    \frac{N(A)}{N} = \frac{1}{n} \left( X_1 + \cdots + X_n \right) = Q_n.
\end{equation}\)
Note that $E(X) = \mathbf{P}(A)$ and $\mathbf{Var}(X) = \mathbf{P}(A) - \mathbf{P}(A)^2$. Therefore, 
\(\begin{equation}
    \frac{N(A)}{N} \to \mathbf{P}(A) \quad \text{ as } n\to\infty.
\end{equation}\)&lt;/p&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
For the law of large numbers to work, $\mathbf{Var}(X)$ must be finite. Otherwise, the law may fail as the following example shows.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; [Cauchy distribution]
The Cauchy distribution is given by
$$\begin{equation}
    f(x) = \frac{1}{\pi (1+x^2)},
\end{equation}$$
where $\pi$ is the normalization parameter. Let $X$ be the random variable which has the Cauchy distribution. Note that although the Cauchy distribution is very like the normal distribution, $X$ doesn&apos;t have the variance. This is because the Cauchy distribution has a long tail as $\left\vert x \right\vert\to\infty$ and it converges slowly. But $X$ has a mean which is $\mu = 0$. So the question is: does $Q_n$ converges to $\mu$? The answer is negative. This example shows that if the variance is not finite, the law of large numbers fails.
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;%&amp;lt;div class=&quot;remark&quot;&amp;gt;&amp;lt;p&amp;gt;&lt;b&gt;Remark.&lt;/b&gt; 
%It is interesting to note that if $X$ and $Y$ are bernoulli, then $X/Y$ is Cauchy.
%&amp;lt;/p&amp;gt;&amp;lt;/div&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;conditional-distributions-and-conditional-expectation&quot;&gt;Conditional distributions and conditional expectation&lt;/h2&gt;
&lt;p&gt;(This section is the supplement of the lecture.)&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 1.&lt;/b&gt; 
The conditional distribution function of $Y$ given $X = x$ is the function $F_{Y\vert X} (\cdot \vert x)$ given by
$$\begin{equation}
    F_{Y | X}(y | x)=\int_{-\infty}^{y} \frac{f(x, v)}{f_{X}(x)} d v
\end{equation}$$
for any $x$ such that $f_X(x) &amp;gt; 0$. It is sometimes denoted $\mathbf{P} (Y \leq y \vert X = x)$. 
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Remembering that distribution functions are integrals of density functions, we are led to the following definition.&lt;/p&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 2.&lt;/b&gt; 
The conditional density function of $F_{Y\vert X}$, written $f_{Y\vert X}$, is given by
$$\begin{equation}
    f_{Y | X}(y | x)=\frac{f(x, y)}{f_{X}(x)} = \frac{f(x, y)}{\int_{-\infty}^{\infty} f(x, y) d y}
\end{equation}$$
for any $x$ such that $f_X(x) &amp;gt; 0$. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 2.&lt;/b&gt; 
The conditional expectation $\psi(X) = \mathbf{E}(Y \vert X)$ satisfies 
$$\begin{equation}
    \mathbf{E}(\psi(X)) = \mathbf{E}(Y).
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 3.&lt;/b&gt; 
The conditional expectation $\psi(X) = \mathbf{E}(Y \vert X)$ satisfies 
$$\begin{equation}
    \mathbf{E}\left( \psi(X) g(X) \right) = \mathbf{E}(Y g(X))
\end{equation}$$
for any function $g$ for which both expectations exist.
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;functions-of-continuous-random-variables&quot;&gt;Functions of continuous random variables&lt;/h2&gt;

&lt;p&gt;Let $X$ be a random variable with density function $f$, and let $g : \mathbb{R} \to \mathbb{R}$ be a sufficiently nice 
function. Then $y = g(X)$ is a random variable also. In order to calculate the distribution of $Y$, we proceed thus
\(\begin{equation}
    \begin{aligned} 
        \mathbf{P}(Y \leq y) &amp;amp;= \mathbf{P}\left(g(X) \leq y\right) = \mathbf{P}\left((g(X) \in(-\infty, y] \right) \\
        &amp;amp;=\mathbf{P}\left(X \in g^{-1}(-\infty, y]\right)=\int_{g^{-1}(-\infty, y]} f(x) d x\end{aligned}.
\end{equation}\)
The $g^{-1}$ is defined as follows. If $A \subseteq \mathbb{R}$ then $g^{-1} A={x \in \mathbb{R}: g(x) \in A}$.&lt;/p&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
Let $g(x) = ax + b$ for fixed $a, b \in \mathbb{R}$. Then $Y = g (X) = aX + b$ has distribution function 
$$\begin{equation}
    \mathbf{P}(Y \leq y) = \mathbf{P}(a X+b \leq y) = \left\{\begin{array}{ll}
    {\mathbf{P}(X \leq(y-b) / a)} &amp;amp; {\text { if } a&amp;gt;0} \\
    {\mathbf{P}(X \geq(y-b) / a)} &amp;amp; {\text { if } a&amp;lt;0}\end{array}\right.
\end{equation}$$
Differentiate to obtain $f_{Y}(y)=|a|^{-1} f_{X}((y-b) / a)$.
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;More generally, if $X_1$ and $X_2$ have joint density function $f$, and $g, h$ are two functions mapping $\mathbb{R}^2 \to \mathbb{R}$, then we can use the Jacobian to find the density the joint density function of the pair $Y_1 = g(X_1 , X_2)$, $Y_2 = h(X_1 , X_2)$.&lt;/p&gt;

&lt;p&gt;Let $y_1 = y_1 (x_1 , x_2)$, $y_2 = y_2(x_1 , x_2)$ 
be a one-one mapping $T : (x_1 , x_2) \mapsto (y_1 , y_2)$ taking some domain $D \subseteq \mathbb{R}^2$ onto some 
range $R \subseteq \mathbb{R}^2$. The transformation can be inverted as $x_1 = x_1(y_1 , y_2)$, $x_2 = x_2(y_1 , y_2)$; the Jacobian of this inverse is defined to be the determinant 
\(\begin{equation}
    J=\left|\begin{array}{ll}{\frac{\partial x_{1}}{\partial y_{1}}} &amp;amp; {\frac{\partial x_{2}}{\partial y_{1}}} \\ {\frac{\partial x_{1}}{\partial y_{2}}} &amp;amp; {\frac{\partial x_{2}}{\partial y_{2}}}\end{array}\right|=\frac{\partial x_{1}}{\partial y_{1}} \frac{\partial x_{2}}{\partial y_{2}}-\frac{\partial x_{1}}{\partial y_{2}} \frac{\partial x_{2}}{\partial y_{1}}
\end{equation}\)
which express as a function $J = J(y_1, y_2)$. We assume the &lt;strong&gt;partial derivatives are continuous&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 4.&lt;/b&gt; 
If $g : \mathbb{R}^2 \to \mathbb{R}$, and $T$ maps the set $A \subseteq D$ onto the set $B \subseteq R$, then 
$$\begin{equation}
    \iint_{A} g\left(x_{1}, x_{2}\right) d x_{1} d x_{2}=\iint_{B} g\left(x_{1}\left(y_{1}, y_{2}\right), x_{2}\left(y_{1}, y_{2}\right)\right)\left|J\left(y_{1}, y_{2}\right)\right| d y_{1} d y_{2}.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Corollary 4.1.&lt;/b&gt; 
If $X_1$, $X_2$ have joint density function $f$, then the pair $Y_1,Y_2$ given by $(Y_1 , Y_2) = T (X_1, X_2)$ has joint density function 
$$\begin{equation}
    f_{Y_{1}, Y_{2}}\left(y_{1}, y_{2}\right)=\left\{\begin{array}{ll}{f\left(x_{1}\left(y_{1}, y_{2}\right), x_{2}\left(y_{1}, y_{2}\right)\right)\left|J\left(y_{1}, y_{2}\right)\right|} &amp;amp; {\text { if }\left(y_{1}, y_{2}\right) \text { is in the range of } T} \\ {0} &amp;amp; {\text { otherwise. }}\end{array}\right.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;A similar result holds for mappings of $\mathbb{R}^n$ into $\mathbb{R}^n$. This technique is sometimes referred to as the method of &lt;strong&gt;change of variables&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
Suppose that 
$$\begin{equation}
    X_{1}=a Y_{1}+b Y_{2}, \quad X_{2}=c Y_{1}+d Y_{2}
\end{equation}$$
where $ad-bc \neq 0$. Check that 
$$\begin{equation}
    f_{Y_{1}, Y_{2}}\left(y_{1}, y_{2}\right) = |a d-b c| f_{X_{1}, X_{2}}\left(a y_{1}+b y_{2}, c y_{1}+d y_{2}\right).
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;multivariate-normal-distribution&quot;&gt;Multivariate normal distribution&lt;/h2&gt;

&lt;h3 id=&quot;definition-and-properties&quot;&gt;Definition and properties&lt;/h3&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 3.&lt;/b&gt; 
The vector $\mathbf{X} = (X_1 , X_2 , \dots , X_n )$ has the **multivariate normal distribution** (or **multinormal distribution**), written $N(\boldsymbol{\mu}, \mathbf{V})$, if its joint density function is 
$$\begin{equation}
    f(\mathbf{x})=\frac{1}{\sqrt{(2 \pi)^{n}|\mathbf{V}|}} \exp \left[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \mathbf{V}^{-1}(\mathbf{x}-\boldsymbol{\mu}) \right], \quad \mathbf{x} \in \mathbb{R}^{n}
\end{equation}$$
where $\mathbf{V}$ is a positive definite symmetric matrix. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 5.&lt;/b&gt; 
If $\mathbf{X}$ is $N(\boldsymbol{\mu}, \mathbf{V})$, then 
&lt;ol&gt;
    &lt;li&gt; $\mathbf{E}(\mathbf{X}) = \boldsymbol{\mu}$, which is to say that $\mathbf{E}(X_i) = \mu_i$ for all $i$,&lt;/li&gt;
    &lt;li&gt; $\mathbf{V} = (v_{ij})$ is called the covariance matrix, because $v_{ij} = \mathbf{cov}(X_i , X_j)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 6.&lt;/b&gt; 
If $\mathbf{X}=\left(X_{1}, X_{2}, \dots, X_{n}\right)$ is $N(\boldsymbol{\mu}, \mathbf{V})$ and $\mathbf{Y}=\left(Y_{1}, Y_{2}, \dots, Y_{m}\right)$ is given by $\mathbf{Y} = \mathbf{XD}$ for some matrix $\mathbf{D}$ of rank $m \leq n$, then $\mathbf{Y}$ is $N\left(\mathbf{0}, \mathbf{D}^T \mathbf{V} \mathbf{D}\right)$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 4.&lt;/b&gt; 
The vector $\mathbf{X}=\left(X_{1}, X_{2}, \dots, X_{n}\right)$ of random variables is said to have the 
**multivariate normal distribution** whenever, for all $\mathbf{a} \in \mathbb{R}^n$, the linear combination $\mathbf{Xa}^T = a_1 X_1 + \dots + a_n X_n$ has a normal distribution.
&lt;/p&gt;&lt;/div&gt;

&lt;h3 id=&quot;distributions-arising-from-the-normal-distribution&quot;&gt;Distributions arising from the normal distribution&lt;/h3&gt;
&lt;p&gt;Suppose that $X_1, X_2, \dots , X_n$ is a collection 
of independent $N(\mu, \sigma^2)$ variables for some fixed but unknown values of $\mu$ and $\sigma^2$. We can use them to estimate $\mu$ and $\sigma^2$.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 5.&lt;/b&gt; 
The **sample mean** of a sequence of random variables $X_1, X_2, \dots , X_n$ is 
$$\begin{equation}
    \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i.
\end{equation}$$
It is usually used as a guess at the value of $\mu$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 6.&lt;/b&gt; 
The **sample variance** of a sequence of random variables $X_1, X_2, \dots , X_n$ is 
$$\begin{equation}
    S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2.
\end{equation}$$
It is usually used as a guess at the value of $\sigma^2$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
The sample mean and the sample variance have the property of being &apos;unbiased&apos; in that $\mathbf{E}(\bar{X}) = \mu$ and $\mathbf{E}(S^2) = \sigma^2$. Note that in some texts the sample variance is defined with $n$ in place of $(n - 1)$. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 7.&lt;/b&gt; 
If $X_1, X_2, \dots , X_n$ are independent $N(\mu, \sigma^2)$ variables, then $\bar{X}$ and $S^2$ are independent. We have that $\bar{X}$ is $N(\mu, \sigma^2/n)$ and $(n-1)S^2 / \sigma^2$ is $\chi^{(n-1)}$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 7.&lt;/b&gt; 
If $X_1, X_2, \dots , X_n$ are standard normal random variables, then the sum of their squares,
$$\begin{equation}
    Q = \sum_{i=1}^n X_i^2
\end{equation}$$
is distributed according to the $\chi^2$ distribution with $n$ **degrees of freedom**. This is usually denoted as
$$\begin{equation}
    Q \sim \chi^2(k) \quad \text{or} \quad Q \sim \chi^2_k.
\end{equation}$$
The probability density function (p.d.f.) of the $\chi^2$ distribution is
$$\begin{equation}
    f(x ; k)=\left\{\begin{array}{ll}
    {\frac{x^{\frac{k}{2}-1} e^{-\frac{x}{2}}}{2^{\frac{k}{2}} \Gamma\left(\frac{k}{2}\right)}} &amp;amp; {x&amp;gt;0} \\ 
    {0} &amp;amp; {\text { otherwise }}\end{array}\right.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;h3 id=&quot;sampling-from-a-distribution&quot;&gt;Sampling from a distribution&lt;/h3&gt;
&lt;p&gt;A basic way of generating a random variable with given distribution function is to use the following theorem.&lt;/p&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 8.&lt;/b&gt; [Inverse transform technique]
Let $F$ be a distribution function, and let $U$ be uniformly distributed on the interval $[0, 1]$. 
&lt;ol&gt;
    &lt;li&gt; If $F$ is a continuous function, the random variable $X = F^{-1} (U)$ has distribution function $F$.&lt;/li&gt;
    &lt;li&gt; &lt;p style=&quot;display: inline;&quot;&gt;Let $F$ be the distribution function of a random variable taking non-negative integer values. The random variable $X$ given by
    $$\begin{equation}
        X = k \quad \text{if and only if} \quad F(k-1) &amp;lt; U \leq F(k)
    \end{equation}$$
    has distribution function $F$. &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;
</description>
        <pubDate>Fri, 30 Oct 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/10/notes-5.html</link>
        <guid isPermaLink="true">http://localhost:4000/2020/10/notes-5.html</guid>
        
        <category>notes</category>
        
        <category>math</category>
        
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Expectation and Dependence</title>
        <description>&lt;h2 id=&quot;expectation-of-discrete-random-variables&quot;&gt;Expectation of discrete random variables&lt;/h2&gt;
&lt;p&gt;The intuition of expectation is the &lt;em&gt;average value&lt;/em&gt; of an experiment. Suppose we do an experiment for $N$ repeated times. The probability of each possible outcome $x$ can be approximately defined by 
\begin{equation}
    \mathbf{P}(x) \approx f(x) = \frac{freq(x)}{N}.
\end{equation}
Then the average outcome is
\begin{equation}
    m \approx \frac{1}{N}\sum_{x} \text{freq}(x)x = \sum_x f(x)x.
\end{equation}&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 1.&lt;/b&gt; 
The &lt;strong&gt;mean value&lt;/strong&gt;, or &lt;strong&gt;expectation&lt;/strong&gt;, or &lt;strong&gt;expected value&lt;/strong&gt; of the random variable $X$ with mass function $f$ is defined to be 
$$\begin{equation}
    \mathbf{E}(X) = \sum_{x:f(x)&amp;gt;0} xf(x)
\end{equation}$$
whenever this sum is absolutely convergent.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
&lt;ol&gt;
    &lt;li&gt;For notation convenience, we also write $\mathbf{E}(X) = \sum_x xf(x)$.&lt;/li&gt;
    &lt;li&gt;We require **absolute convergence** in order that $\mathbf{E}(X)$ be unchanged by reordering the $x_i$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 1.&lt;/b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Riemann_series_theorem&quot;&gt; Riemann Rearrangement Theorem&lt;/a&gt;.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
If $X$ has mass function $f$ and $g:\mathbb{R}\to\mathbb{R}$, then
$$\begin{equation}
    \mathbf{E}(g(x)) = \sum_x g(x) f(x)
\end{equation}$$
whenever this sum is absolutely convergent.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
If $X$ is a random variable with mass function f, and $g(x) = x^2$, then 
$$\begin{equation}
    \mathbf{E}(X^2) = \sum_x g(x)f(x) = x^2 f(x).
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 2.&lt;/b&gt; 
If $k$ is a positive integer, the $k$th &lt;b&gt;moment&lt;/b&gt; $m_k$ of $X$ is defined to be 
\begin{equation}
    m_k = \mathbf{E}(X^k).
\end{equation}
The $k$th &lt;b&gt;central moment&lt;/b&gt; $\sigma_k$ is defined as
\begin{equation}
    \sigma_k = \mathbf{E}\left( (X-m_1)^k \right) = \mathbf{E}\left( (X-E(X))^k \right).
\end{equation}
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;The two moments of most use are $m_1 = \mathbf{E}(X)$ and $\sigma_2 = \mathbf{E}( (X - \mathbf{E}(X))^2$, called the &lt;strong&gt;mean&lt;/strong&gt; (or &lt;strong&gt;expectation)&lt;/strong&gt; and &lt;strong&gt;variance&lt;/strong&gt; of $X$. These two quantities are measures of the mean and dispersion of $X$; that is, $m_1$ is the average value of $X$, and $\sigma_2$ measures the amount by which $X$ tends to deviate from this average. The mean $m_1$ is often denoted $\mu$, and the variance of $X$ is often denoted $\mathbf{Var}(X)$. The positive square root $\sigma = \mathbf{Var}(X)$ is called the &lt;strong&gt;standard deviation&lt;/strong&gt;, and in this notation $\sigma_2 = \sigma^2$.&lt;/p&gt;

&lt;p&gt;The central moments ${\sigma_i}$ can be expressed in terms of the ordinary moments ${m_i}$. For example, $\sigma_1 = 0$, and&lt;/p&gt;

\[\begin{equation}
    \begin{aligned} 
        \sigma_{2} &amp;amp;=\sum_{x}\left(x-m_{1}\right)^{2} f(x) \\ 
        &amp;amp;=\sum_{x} x^{2} f(x)-2 m_{1} \sum_{x} x f(x)+m_{1}^{2} \sum_{x} f(x) \\ 
        &amp;amp;=m_{2}-m_{1}^{2} ,
    \end{aligned}
\end{equation}\]

&lt;p&gt;which may be written as 
\(\begin{equation}
    \tag{4-1}
    \mathbf{Var}(X) = \mathbf{E}\left( (X-E(X))^2 \right) = \mathbf{E}(X^2) - \mathbf{E}(X)^2.
\end{equation}\)&lt;/p&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; [Binomial variables]
Let $X$ be a random variable with binomial distribution. The p.m.f. is 
$$\begin{equation}
    f(k) = \binom{n}{k} p^k q^{n-k} \quad k = 0,\dots, n,
\end{equation}$$
where $q = 1-p$. The expectation of $X$ is
$$\begin{equation}
    \mathbf{E}(X) = \sum_{k=0}^n k f(k) = \sum_{k=0}^n k\binom{n}{k} p^k q^{n-k}.
\end{equation}$$
We use the following algebraic identity to compute $\mathbf{E}(X)$.
$$\begin{equation}
    \label{eq:4.2}
    \tag{4-2}
    \sum_{k=0}^n \binom{n}{k} x^k = (1+x)^n, 
\end{equation}$$
Differentiate it and multiply by $x$, we obtain 
$$\begin{equation}
    \label{eq:4.3}
    \tag{4-3}
    \sum_{k=0}^n k \binom{n}{k} x^k = nx(1+x)^{n-1}. 
\end{equation}$$
We substitute $x = p / q$ to obtain $\mathbf{E}(X) = np$. A similar argument shows that the variance of $X$ is given by $\mathbf{Var}(X) = npq$. 
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;We can think of the process of calculating expectations as a &lt;strong&gt;linear operator&lt;/strong&gt; on the space of random variables.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 2.&lt;/b&gt; 
The expectation operator $\mathbf{E}$ has the following properties: 
&lt;ol&gt;
    &lt;li&gt; if $X\geq 0$, then $\mathbf{E}(X) \geq 0$,&lt;/li&gt;
    &lt;li&gt; if $a, b \in \mathbb{R}$, then $\mathbf{E}(aX+bY) = a\mathbf{E}(X) + b\mathbf{E}(Y)$,&lt;/li&gt;
    &lt;li&gt; the random variable 1, taking the value 1 always, has expectation $\mathbf{E}(1) = 1$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf1-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf1-content&quot;&gt;&lt;p&gt;
We only prove the second property, which is also called the linear property.

We must use the joint p.m.f. of $X$ and $Y$ to compute the expectation. 
$$\begin{equation}
    \begin{split}
        \mathbf{E}(aX+bY) &amp;amp;= \sum_{i, j} (ax_i + by_j) f(x_i, y_j) \\
        &amp;amp;= a \sum_{i,j} x_i f(x_i, y_j) + b\sum_{i,j} y_j f(x_i, y_j) \\
        &amp;amp;= a\sum_{i} x_i f_X(x_i) + b\sum_{j}y_j f_Y(y_j) \\ 
        &amp;amp;= a\mathbf{E}(X) + b\mathbf{E}(Y),
    \end{split}
\end{equation}$$
where $f_X(x)$ and $f_Y(y)$ are marginal p.m.f. of $X$ and $Y$ respectively.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf1&quot;);&lt;/script&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
It is &lt;b&gt;NOT&lt;/b&gt; in general true that $\mathbf{E}(XY)$ is the same as $\mathbf{E}(X)\mathbf{E}(Y)$. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
If $X$ and $Y$ are independent, then $\mathbf{E}(XY) = \mathbf{E}(X)\mathbf{E}(Y)$. 
&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf2-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf2-content&quot;&gt;&lt;p&gt;
If $X, Y$ are independent, $f(x,y) = f_X(x) f_Y(y)$. Then
$$\begin{equation}
    \mathbf{E}(XY) = \sum_{ij} x_i y_j f(x,y) = \sum_{i} \left( x_i f_X(x_i) \right) \sum_{j} \left( y_j f_Y(y_j) \right) = \mathbf{E}(X) \mathbf{E}(Y).
\end{equation}$$
\end{proof}
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf2&quot;);&lt;/script&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 3.&lt;/b&gt; 
$X$ and $Y$ are called **uncorrelated** if $\mathbf{E}(XY) = \mathbf{E}(X)\mathbf{E}(Y)$.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Independent variables are uncorrelated. But the converse is **NOT** true.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 3.&lt;/b&gt; 
For random variables $X$ and $Y$, 
&lt;ol&gt;
    &lt;li&gt; $\mathbf{Var}(aX) = a^2 \mathbf{Var}(X)$ for $a \in \mathbb{R}$,&lt;/li&gt;
    &lt;li&gt; $\mathbf{Var}(X+Y) = \mathbf{Var}(X) + \mathbf{Var}(Y)$ is $X$ and $Y$ are uncorrelated.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
The above theorem shows that the variance operator $\mathbf{Var}$ is **NOT** a linear operator, even when it 
is applied only to uncorrelated variables. 
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Sometimes the sum $S = \sum xf(x)$ does not converge absolutely, which means the mean of the distribution does not exist. Here is an example.&lt;/p&gt;
&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; [A distribution without a mean] Let $X$ have mass function 
$$\begin{equation}
    f(k) = Ak^{-1} \quad k = \pm 1, \pm 2, \dots,
\end{equation}$$
where $A$ is chosen so that $\sum_k f(k) = 1$. The sum $\sum_k kf(k) = A\sum_{k\neq 0} k^{-1}$ doesn&apos;t converge absolutely, because both the positive and the negative parts diverge. 
&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;This example is suitable to point out that we can base probability theory upon the expectation operator $\mathbf{E}$ rather than upon the probability measure $\mathbf{P}$. Roughly speaking, the way we proceed is to postulate axioms, such as (a)-(c) of the above Theorem, for a so-called “expectation operator” $\mathbf{E}$ acting on a space of ``random variables”. The probability of an event can then be recaptured by defining $\mathbf{P}(A) = \mathbf{E}(I_A)$.&lt;/p&gt;

&lt;p&gt;Recall the indicator function of a set $A$ is defined as
\(\begin{equation}
    I_A(\omega) = \begin{cases} 1 &amp;amp; \omega \in A, \\
    0 &amp;amp; \omega \not\in A. \end{cases}
\end{equation}\)
In addition, we have $\mathbf{E}(I_A) = \mathbf{P}(A)$.&lt;/p&gt;

&lt;h2 id=&quot;dependence-of-discrete-random-variables&quot;&gt;Dependence of discrete random variables&lt;/h2&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 4.&lt;/b&gt; 
The **joint distribution function** $F:\mathbb{R}^2 \to [0,1]$ of $X$ and $Y$, where $X$ and $Y$ are discrete variables, is given by 
$$\begin{equation}
    F(x, y) = \mathbf{P}(X\leq x \text{ and } Y \leq y). 
\end{equation}$$
Their **joint mass function** $f:\mathbb{R}^2 \to [0,1]$ is given by 
$$\begin{equation}
    f(x,y) = \mathbf{P}(X = x \text{ and } Y = y). 
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;We write $F_{X,Y}$ and $f_{X,Y}$ when we need to stress the role of $X$ and $Y$. We may think of the joint mass function in the following way. If $A_x = {X = x}$ and $B_y = {Y = y}$, then 
\(\begin{equation}
    f(x,y) = \mathbf{P}(A_x \cap B_y).
\end{equation}\)&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
The discrete random variables $X$ and $Y$ are **independent** if and only if 
$$\begin{equation}
    \tag{4-4}
    f_{X,Y}(x,y) = f_X(x)f_Y(y) \quad \forall x,y \in \mathbb{R}.
\end{equation}$$
More generally, $X$ and $Y$ are independent if and only if $f_{X,Y}(x, y)$ can be **factorized as the product** $g(x)h (y)$ of a function of $x$ alone and a function of $y$ alone. 
&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
We stress that the factorization Eq.(4-4) must hold for all $x$ and $y$ in order that $X$ and $Y$ be independent. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
$$\begin{equation}
    \mathbf{E}(g(X, Y))=\sum_{x, y} g(x, y) f_{X, Y}(x, y).
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 5.&lt;/b&gt; 
The covariance of $X$ and $Y$ is
$$\begin{equation}
    \mathbf{cov}(X,Y) = \mathbf{E}\left( (X-\mathbf{E}(X))(Y-\mathbf{E}(Y)) \right).
\end{equation}$$
The correlation (coefficient) of $X$ and $Y$ is 
$$\begin{equation}
\mathbf{corr}(X, Y) = \rho(X,Y) = \frac{\mathbf{cov}(X,Y)}{\sqrt{\mathbf{Var}(X)\mathbf{Var}(Y)}}
\end{equation}$$
as long as the variances are non-zero. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Notice the following two equations.
&lt;ol&gt;
    &lt;li&gt; $\mathbf{cov}(X,X) = \mathbf{Var}(X)$,&lt;/li&gt;
    &lt;li&gt; $\mathbf{cov}(X,Y) = \mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Covariance itself is not a satisfactory measure of dependence because the scale of values which $\mathbf{cov}(X, Y)$ may take contains no points which are clearly interpretable in terms of the relationship between $X$ and $Y$.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 4.&lt;/b&gt; [Cauchy-Schwarz inequality] For random variables $X$ and $Y$, 
$$\begin{equation}
    \mathbf{E}(XY)^2 \leq \mathbf{E}(X^2) \mathbf{E}(Y^2)
\end{equation}$$ 
with equality if and only if $\mathbf{P}(aX = bY) = 1$ for some real $a$ and $b$, at least one of which is non-zero. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;pf3-link&quot;&gt;&lt;b&gt;Proof &amp;#9656;&lt;/b&gt;&lt;/a&gt;
&lt;div class=&quot;proof-content&quot; id=&quot;pf3-content&quot;&gt;&lt;p&gt;
For $a, b \in \mathbb{R}$, let $Z = aX - bY$. Then 
$$\begin{equation}
    0 \leq \mathbf{E}(Z^2) = a^2 \mathbf{E}(X^2) - 2ab\mathbf{E}(XY) + b^2\mathbf{E}(Y^2).
\end{equation}$$
Thus the right-hand side is a quadratic in the variable $a$ with at most one real root. Its discriminant must be non-positive. That is to say, if $b \neq 0$, 
$$\begin{equation}
    \mathbf{E}(XY)^2 - \mathbf{E}(X^2) \mathbf{E}(Y^2) \leq 0. 
\end{equation}$$
The discriminant is zero if and only if the quadratic has a real root. This occurs if and only if 
$$\begin{equation}
    \mathbf{E}\left( (aX-bY)^2 \right) = 0
\end{equation}$$
for some $a$ and $b$.
&amp;#9724;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;toggle_proof(&quot;pf3&quot;);&lt;/script&gt;

&lt;p&gt;We define $X’ = X-\mathbf{E}(X), Y’ = Y - \mathbf{E}(Y)$. Since all $X, Y$ satisfy the Cauchy-Schwarz inequality, so do $X’$ and $Y’$. Therefore, 
\(\begin{equation}
    \mathbf{E}(X&apos;Y&apos;)^2 \leq \mathbf{E}(X&apos;^2) \mathbf{E}(Y&apos;^2) \quad \Leftrightarrow \quad \mathbf{cov}(X, Y)^2 \leq \mathbf{Var}(X)\mathbf{Var}(Y).
\end{equation}\)
Therefore, 
\(\begin{equation}
    \rho(X,Y)^2 \leq 1 \quad \mathbb{R}ightarrow \quad \rho(X,Y) \in [-1, 1].
\end{equation}\)
which gives the following lemma.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
The correlation coefficient $\rho$ satisfies $\left\vert \rho (X, Y)  \right\vert \leq 1$ with equality if and only if $\mathbf{P}(aX + bY = c) = 1$ for some $a, b, c \in \mathbb{R}$. 
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;expectation-of-continuous-random-variables&quot;&gt;Expectation of continuous random variables&lt;/h2&gt;
&lt;h3 id=&quot;idea-of-translating-expectation-from-discrete-to-continuous&quot;&gt;Idea of translating expectation from discrete to continuous&lt;/h3&gt;
&lt;p&gt;Suppose we have a continuous random variable $X$ with $f$ being the probability density function. We split $X$ into small intervals $\Delta x$. Then $p_i = f(x_i)\Delta x$. $\frac{p_i}{\Delta x}$ is an approximation of probability density function. Therefore, 
\(\begin{equation}
    \mathbf{E}(X) \approx \sum_{i} x_i p_i = \sum_{i} x_i f(x_i) \Delta x,
\end{equation}\)
which is the Remann sum. We take the limit and get
\(\begin{equation}
    \mathbf{E}(x) = \int_{-\infty}^\infty x f(x)dx.
\end{equation}\)&lt;/p&gt;

&lt;h3 id=&quot;expectation&quot;&gt;Expectation&lt;/h3&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 6.&lt;/b&gt; 
The **expectation** of a continuous random variable $X$ with density function $f$ is given by 
$$\begin{equation}
    \mathbf{E}(X) = \int_{-\infty}^\infty xf(x) dx
\end{equation}$$
whenever this integral exists.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 5.&lt;/b&gt; 
If $X$ and $g(X)$ are continuous random variables, then $$\begin{equation}
    \mathbf{E}\left( g(X) \right) = \int_{-\infty}^\infty g(x)f(x) dx.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 7.&lt;/b&gt; 
The $k$th **moment** of a continuous variable $X$ is defined as
$$\begin{equation}
    \mathbf{E}(X^k) = \int_{-\infty}^\infty x^k f(x) dx
\end{equation}$$
whenever the integral converges.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; [Cauchy distribution] The random variable $X$ has the Cauchy distribution t if it has density 
function 
$$\begin{equation}
    f(x) = \frac{1}{\pi (1+x^2)}, \quad x \in \mathbb{R}.
\end{equation}$$
This distribution is notable for having no moments.
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;dependence-of-continuous-random-variables&quot;&gt;Dependence of continuous random variables&lt;/h2&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 8.&lt;/b&gt; 
The **joint distribution function** of $X$ and $Y$ is the function $F: \mathbb{R}^2 \to [0, 1]$ given by 
$$\begin{equation}
    F(x,y) = \mathbf{P}(X\leq x, Y \leq y).
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 9.&lt;/b&gt; 
The random variables $X$ and $Y$ are **(jointly) continuous** with **joint (probability) density function** $f : \mathbb{R}^2 \to [0, \infty)$ if
$$\begin{equation}
    F(x, y)=\int_{v=-\infty}^{y} \int_{u=-\infty}^{x} f(u, v) d u d v \quad \text{for each } x, y\in\mathbb{R}. 
\end{equation}$$
If $F$ is sufficiently differentiable at the point $(x , y)$, then we usually specify 
$$\begin{equation}
    f(x, y)=\frac{\partial^{2}}{\partial x \partial y} F(x, y).
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Probabilities&lt;/strong&gt;:&lt;/p&gt;

\[\begin{equation}
    \begin{aligned} 
        \mathbf{P}(a \leq X \leq b, c \leq Y \leq d) &amp;amp;=F(b, d)-F(a, d)-F(b, c)+F(a, c) \\ 
        &amp;amp;=\int_{y=c}^{d} \int_{x=a}^{b} f(x, y) d x d y. \end{aligned}
\end{equation}\]

&lt;p&gt;If $B$ is a sufficiently nice subset of $\mathbb{R}^2$, then
\(\begin{equation}
    \mathbf{P} \left( (X, Y) \in B \right)=\iint_{B} f(x, y) d x d y.
\end{equation}\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Marginal distributions&lt;/strong&gt;: The marginal distribution functions of $X$ and $Y$ are&lt;/p&gt;

\[\begin{equation}
    F_{X}(x)=\mathbf{P}(X \leq x)=F(x, \infty), \quad F_{Y}(y)=\mathbf{P}(Y \leq y)=F(\infty, y). 
\end{equation}\]

\[\begin{equation}
    F_{X}(x)=\int_{-\infty}^{x}\left(\int_{-\infty}^{\infty} f(u, y) d y\right) d u.
\end{equation}\]

&lt;p&gt;Marginal density function of $X$ and $Y$:
\(\begin{equation}
    f_{X}(x)=\int_{-\infty}^{\infty} f(x, y) d y, \quad f_{Y}(y)=\int_{-\infty}^{\infty} f(x, y) d x.
\end{equation}\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expectation&lt;/strong&gt;: If $g: \mathbb{R}^2 \to \mathbb{R}$ is a sufficiently nice function, then&lt;/p&gt;

\[\begin{equation}
    \mathbf{E}(g(X, Y))=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f(x, y) d x d y;
\end{equation}\]

&lt;p&gt;in particular, setting $g(x, y) = ax + by$,&lt;/p&gt;

\[\begin{equation}
    \mathbf{E}(aX+bY) = a\mathbf{E}(X) + b\mathbf{E}(Y).
\end{equation}\]

&lt;p&gt;&lt;strong&gt;Independence&lt;/strong&gt;: The random variables $X$ and $Y$ are independent if and only if&lt;/p&gt;

\[\begin{equation}
    F(x,y) = F_X(x) F_Y(y) \quad \forall x, y \in \mathbb{R},
\end{equation}\]

&lt;p&gt;which, for &lt;strong&gt;continuous random variables&lt;/strong&gt;, is equivalent to requiring that&lt;/p&gt;

\[\begin{equation}
    f(x,y) = f_X(x) f_Y(y).
\end{equation}\]

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 6.&lt;/b&gt; [Cauchy-Schwarz inequality] For any pair $X, Y$ of jointly continuous variables, we have that 
$$\begin{equation}
    \mathbf{E}(XY)^2 \leq \mathbf{E}(X^2) \mathbf{E}(Y^2), 
\end{equation}$$
with equality if and only if $\mathbf{P}(aX = bY) = 1$ for some real $a$ and $b$, at least one of which is non-zero. 
&lt;/p&gt;&lt;/div&gt;
</description>
        <pubDate>Fri, 23 Oct 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/10/notes-4.html</link>
        <guid isPermaLink="true">http://localhost:4000/2020/10/notes-4.html</guid>
        
        <category>notes</category>
        
        <category>math</category>
        
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Continuous Randon Variables</title>
        <description>&lt;h2 id=&quot;probability-mass-functions&quot;&gt;Probability mass functions&lt;/h2&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 1.&lt;/b&gt; 
The &lt;b&gt;(probability) mass function&lt;/b&gt; (p.m.f.) of a discrete random variable $X$ is the function $f: \mathbb{R} \to[0, 1]$ given by $f(x) = \mathbf{P}(X = x)$. 
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Notice that $f(x)$ is not discontinuous. The distribution and mass functions are related by
\(\begin{equation}
    F(x) = \sum_{i: x_i \leq x} f(x_i), \quad f(x) = F(x) - \lim_{y\uparrow x} F(y).
\end{equation}\)&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
The probability mass function $f: \mathbb{R} \to [0, 1]$ satisfies: 
&lt;ol&gt;
    &lt;li&gt;the set of $x$ such that $f(x) \neq 0$ is countable,&lt;/li&gt;
    &lt;li&gt;$\sum_{i} f(x_i) = 1$, where $x_1, x_2, \dots$ are the values of $x$ such that $f(x)\neq 0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
&lt;b&gt;Binomial distribution.&lt;/b&gt; A coin is tossed $n$ times, and a head turns up each time with probability $p (= 1 - q)$. Then $\Omega = \{H, T\}^n \}$. The total number $X$ of heads takes values in the set $\{0, 1, 2, \dots , n\}$ and is a discrete random variable. Its probability mass function $f(x) = \mathbf{P}(X = x)$ satisfies 
$$\begin{equation}
    f(k)= \binom{n}{k} p^{k}(1-p)^{n-k}, \quad k=0,1, \ldots, n.
\end{equation}$$
The random variable $X$ is said to have the &lt;b&gt;binomial distribution&lt;/b&gt; with parameters $n$ and $p$, It is the sum $X = Y_1 + Y_2 + \dots + Y_n$ of $n$ Bernoulli variables.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
&lt;b&gt;Poisson distribution.&lt;/b&gt; If a random variable $X$ takes values in the set $\{O, 1, 2, \dots \}$ with mass function
$$\begin{equation}
    f(k) = \frac{\lambda^k}{k!} e^{-\lambda}, \quad k = 0, 1, 2, \dots, 
\end{equation}$$
where $\lambda &amp;gt; 0$, then $X$ is said to have the &lt;b&gt;Poisson distribution&lt;/b&gt; with parameter $\lambda$. Figure 1. shows how p.m.f varies with $k$.
&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/blog/2020/2020-10-16-poisson.png&quot; height=&quot;230&quot; /&gt;
    &lt;figcaption&gt;Fig.1: p.m.f of Poisson distribution.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
The interpretation of Poisson distribution can be found via this link: &lt;a href=&quot;https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459&quot;&gt;The Poisson Distribution and Poisson Process Explained&lt;/a&gt;.
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;independence-of-discrete-random-variables&quot;&gt;Independence of discrete random variables&lt;/h2&gt;
&lt;p&gt;Recall that events $A$ and $B$ are called independent if and only if $\mathbf{P}(A \cap B) = \mathbf{P}(A)\mathbf{P}(B)$.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 2.&lt;/b&gt; 
Discrete variables $X$ and $Y$ are &lt;b&gt;independent&lt;/b&gt; if the events $\{X = x\}$ and $\{Y = y\}$ are independent for &lt;b&gt;all&lt;/b&gt; $x$ and $y$. 
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Let $A: = { \omega \;\vert\; X(\omega) = x }$ and $B:= { \omega \;\vert\; Y(\omega) = y }$. Then we can write $P(A) = f_X(x), P(B) = f_Y(y)$ and $P(A \cap B) = f(x, y)$. Therefore, $A$ and $B$ are independent if and only if $f(x,y) = f_X(x) f_Y(y)$ for all $x$ and $y$.&lt;/p&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
The equality $f(x,y) = f_X(x) f_Y(y)$ can be used as the criterion to determine whether two discrete random variables $X$ and $Y$ are independent or not. But &lt;b&gt;we need to be careful with it when dealing with continuous random variables&lt;/b&gt; as there will be additional assumptions to determine the independence of continuous random variables using this equality.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Theorem 1.&lt;/b&gt; 
If $X$ and $Y$ are independent and $g, h: \mathbb{R} \to \mathbb{R}$, then $g(X)$ and $h(Y)$ are independent also. 
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;More generally, we say that a family ${ X_i \;\vert\; i \in I}$ of (discrete) random variables is independent if the events ${X_i = x_i }, i \in I$, are independent for all possible choices of the set ${x_i \;\vert\; i \in I}$ of the values of the $X_i$ . That is to say, ${X_i \;\vert\; i \in I }$ is an independent family if and only if 
\(\begin{equation}
    \mathbf{P}(X_i = x_i \text{ for all }i\in J) = \prod_{i\in J} \mathbf{P}(X_i = x_i)
\end{equation}\)
for all sets ${x_i \;\vert\; i \in I}$ and for all finite subsets $J$ of $I$.&lt;/p&gt;

&lt;h2 id=&quot;probability-density-functions&quot;&gt;Probability density functions&lt;/h2&gt;
&lt;p&gt;Recall that a random variable $X$ is continuous if its distribution function $F(x) = \mathbf{P}(X \leq x)$ can be written as\footnote{This is just a general integral, $f(u)$ may or may not be continuous.}
\(\begin{equation}
    F(x) = \int_{\infty}^x f(u)du
\end{equation}\)
for some integrable $f: \mathbb{R} \to [0, \infty)$.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 3.&lt;/b&gt; 
The function $f$ is called the &lt;b&gt;(probability) density function&lt;/b&gt; (p.d.f.) of the continuous random variable $X$. 
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
The function $f$ is &lt;b&gt;NOT&lt;/b&gt; unique. We can add some separate points or a countable set of points which has zero measure to $f$. This doesn&apos;t change the value of the integral. However, if $F$ is differentiable at $u$ then we shall normally set $f(u) = F&apos;(u)$. 
&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Next, we assume $f$ is continuous, then from the basic theorem of calculus, $F$ must be differentiable. Recall 
\(\begin{equation}
    P(a &amp;lt; X(\omega) \leq b) = F(b) - F(a) = \int_{a}^b f(u)du.
\end{equation}\)
Then $P(X=x) = F(x) - \lim_{y \uparrow x} F(y)$. $F$ is &lt;strong&gt;absolutely continuous&lt;/strong&gt; for continuous random variables. Thus we have
\(\begin{equation}
    \lim_{y\uparrow x} F(y) = F(x) \quad \mathbb{R}ightarrow \quad P(X=x) = 0 \quad \forall x\in\mathbb{R}.
\end{equation}\)
This means &lt;strong&gt;the probability of a continuous random variable $X$ taking value at a certain point is 0&lt;/strong&gt;. Very roughly speaking, this lies in the observation that there are uncountably many possible values for $X$; this number is so large that the probability of $X$ taking any particular value cannot exceed zero.&lt;/p&gt;

&lt;p&gt;The numerical value $f(x)$ is &lt;strong&gt;NOT&lt;/strong&gt; a probability. Check the probability $\mathbf{P}(x &amp;lt; X \leq x + dx)$ for a very small $dx$:
\(\begin{equation}
    \mathbf{P}(x &amp;lt; X \leq x + dx) = F(x + dx) - F(x) \approx f(x) dx. 
\end{equation}\)
Since $dx$ is a very small interval rather a number, we cannot say $f(x)$ is the probability of something.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Lemma.&lt;/b&gt; 
If $X$ has density function $f$, then
&lt;ol&gt;
    &lt;li&gt; $\int_{-\infty}^\infty f(x)dx = 1$,&lt;/li&gt;
    &lt;li&gt; $\mathbf{P}(X=x) = 0$ for all $x \in \mathbb{R}$,&lt;/li&gt;
    &lt;li&gt; $\mathbf{P}(a &amp;lt; X \leq b) = \int_a^b f(x)dx$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;independence-of-continuous-random-variables&quot;&gt;Independence of continuous random variables&lt;/h2&gt;
&lt;h3 id=&quot;independence-of-general-random-variables&quot;&gt;Independence of general random variables&lt;/h3&gt;
&lt;div class=&quot;theorem&quot;&gt;&lt;p&gt;&lt;b&gt;Definition 4.&lt;/b&gt; 
Random variables $X$ and $Y$ are called &lt;b&gt;independent&lt;/b&gt; if $\{ X \leq x\}$ and $\{Y \leq y\}$ are independent events for all $x, y \in \mathbb{R}$. 
&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;Note that this definition is the general definition of the independence of any two variables $X$ and $Y$ regardless of their types. The independence of discrete random variables is included in this definition.&lt;/p&gt;

&lt;p&gt;Recall the marginalization. If two random variables $X$ and $Y$ are independent, we have
\(\begin{equation}
    \mathbf{P}(X\leq x) = \lim_{y\to\infty}F(x, y) \equiv F_X(x), \quad 
    \mathbf{P}(Y \leq y) = \lim_{x\to\infty}F(x, y) \equiv F_Y(y).
\end{equation}\)
Therefore,
\(\begin{equation}
    \tag{1}
    F(x,y) = \mathbf{P}(X \leq x, Y \leq y) = \mathbf{P}(X\leq x) \mathbf{P}(Y \leq y) = F_X(x) F_Y(y). 
\end{equation}\)
Note that we are dealing with distribution function in Eq.(1). Eq.(1) can be used as the general criterion to determine whether two random variables are independent or not.&lt;/p&gt;

&lt;h3 id=&quot;independence-of-continuous-variables&quot;&gt;Independence of continuous variables&lt;/h3&gt;
&lt;p&gt;If $X, Y$ are continuous, then we have
\(\begin{equation}
    F(x, y) = \int_{-\infty}^x \int_{-\infty}^y f(x,y) dx dy.
\end{equation}\)
\(\begin{equation}
    F_X(x) = \int_{-\infty}^x \int_{-\infty}^\infty f(x, y)dx dy \quad \mathbb{R}ightarrow \quad f_X(x) = \int_{-\infty}^\infty f(x,y)dy,
\end{equation}\)
where $f_X(x)$ is called the &lt;strong&gt;marginal probability density function&lt;/strong&gt; of $X$. Similarly, we can define $F_Y(y)$ and $f_Y(y)$.&lt;/p&gt;

&lt;p&gt;Assume $f(x,y)$ is continuous, then $F(x,y)$ must be twice differentiable w.r.t. $x$ and $y$. Therefore, from \eqref{eq:&lt;em&gt;}, we can derive a *very practical criterion&lt;/em&gt; to determine the independence of two continuous random variables:
\(\begin{equation}
    \tag{2}
    f(x,y) = \frac{\partial^2 F(x,y)}{\partial x \partial y} = \frac{\partial}{\partial x \partial y} F_X(x) F_Y(y) = f_X(x) f_Y(y).
\end{equation}\)&lt;/p&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
Note that the prerequisite of Eq.(2) is that $f(x,y)$ is continuous.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
&lt;b&gt;Uniform distribution.&lt;/b&gt; The random variable $X$ is uniform on $[a, b]$ if it has distribution function
$$\begin{equation}
    F(x) = \begin{cases} 
        0 &amp;amp; x \leq x, \\ \frac{x-a}{b-a} &amp;amp; a &amp;lt; x \leq b, \\ 1 &amp;amp; x &amp;gt; b
    \end{cases}.
\end{equation}$$
The density function is
$$\begin{equation}
    f(x) = \begin{cases} \frac{1}{b-a} &amp;amp; a &amp;lt; x \leq b \\ 0 &amp;amp; o.w. \end{cases}
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
&lt;b&gt;Exponential distribution&lt;/b&gt;. The random variable $X$ is exponential with parameter $\lambda(&amp;gt; 0)$ if it has distribution function 
$$\begin{equation}
    F(x) = 1 - e^{-\lambda x}, \quad x \geq 0.
\end{equation}$$
The density function is
$$\begin{equation}
    f(x) = \begin{cases} \lambda e^{-\lambda x} &amp;amp; x &amp;gt; 0 \\ 0 &amp;amp; o.w.
    \end{cases}
\end{equation}$$
Note that $F(x)$ is not differentiable at $x=0$. This means $f$ has discontinuity at $x=0$. Thus we need to choose some value for $f(0)$. It doesn&apos;t matter what value we choose as it doesn&apos;t affect the integral. Figure 2 shows the p.d.f. of Exponential distribution.
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/blog/2020/2020-10-16-exponential.png&quot; height=&quot;300&quot; /&gt;
    &lt;figcaption&gt;Fig.2: p.m.f of exponential distribution.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
The interpretation of Exponential distribution can be found via &lt;a href=&quot;https://www.probabilitycourse.com/chapter4/4_2_2_exponential.php&quot;&gt;Exponential distribution 1&lt;/a&gt; and &lt;a href=&quot;https://www.statlect.com/probability-distributions/exponential-distribution&quot;&gt;Exponential distribution 2&lt;/a&gt;. Pay attention to the connection between Exponential distribution and Poisson distribution.
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;example&quot;&gt;&lt;p&gt;&lt;b&gt;Example.&lt;/b&gt; 
&lt;b&gt;Normal (or Gaussian) distribution.&lt;/b&gt; The most important continuous distribution, which has two parameters $\mu$ and $\sigma^2$ and density function
$$\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x-\mu)^2}{2\sigma^2} \right), \quad x\in\mathbb{R}. 
\end{equation}$$
It is denoted by $N(\mu, \sigma^2)$. If $\mu=0$ and $\sigma^2=1$, then
$$\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}, \quad x\in\mathbb{R}.
\end{equation}$$
is the density of the standard normal distribution. It is easy to generalize 2D case to multivariable case. Suppose $\mathbf{x} \in \mathbb{R}^n$, $\mathbf{\mu} \in \mathbb{R}^n$ is the mean vector and $\mathbf{\sigma}^2 \in \mathbb{R}^{n\times n}$ is the covariance matrix. Then
$$\begin{equation}
    f(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^n} \det(\mathbf{\sigma}^2)} \exp \left( -\frac{1}{2} (\mathbf{x}-\mathbf{\mu})^T (\mathbf{\sigma}^2)^{-1} (\mathbf{x}-\mathbf{\mu}) \right), \quad \mathbf{x} \in \mathbb{R}^n.
\end{equation}$$
&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;&lt;p&gt;&lt;b&gt;Remark.&lt;/b&gt; 
For 2D case, if $\sigma^2$ is diagonal, then $f(x,y) = f(x)f(y)$. Therefore, $\sigma^2$ is a measure of independence. For multivariable cases, if $\sigma^2$ is diagonal, then all random variables are independent with each other.
&lt;/p&gt;&lt;/div&gt;

</description>
        <pubDate>Fri, 16 Oct 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/10/notes-3.html</link>
        <guid isPermaLink="true">http://localhost:4000/2020/10/notes-3.html</guid>
        
        <category>notes</category>
        
        <category>math</category>
        
        
        <category>Probability</category>
        
      </item>
    
  </channel>
</rss>
